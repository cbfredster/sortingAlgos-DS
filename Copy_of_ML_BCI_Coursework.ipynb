{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbfredster/sortingAlgos-DS/blob/main/Copy_of_ML_BCI_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTxMfmE3MHJy"
      },
      "source": [
        "# **Multimodal Machine Learning with Brain, Image, and Text Data**\n",
        "\n",
        "This notebook provides a comprehensive guide to training machine learning models using multimodal data, including brain signals, images, and text. You will explore various data processing techniques, model training using neural networks and traditional classifiers, and performance evaluation using metrics like accuracy, precision, and recall. The goal is to integrate diverse data sources to improve classification performance and gain deeper insights into complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kodAZfDg6jY2"
      },
      "source": [
        "# **Install the package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFw_r5-5DhAB"
      },
      "source": [
        "##**1. Install the data reading package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skx0DtiRDmzE"
      },
      "source": [
        "**Library Installation**:\n",
        "\n",
        "The code first installs the required library, mmbra, using the command pip install mmbra, and pip install mmbracategories. This step ensures that all the necessary dependencies for this specific library are available in the environment. The installation process is critical for using the functionalities provided by the mmbra package and mmbracategories package in subsequent steps.\n",
        "\n",
        "**Library Import**:\n",
        "\n",
        "After the installation, imports the mmbra module and mmbracategories module. This import statement makes the library's functions and classes accessible within the code, allowing for seamless integration with other operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHxCL7ebQwli",
        "outputId": "bdc8e267-8472-4314-e42f-58de45a43b24",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmbra\n",
            "  Downloading mmbra-0.1-py3-none-any.whl.metadata (188 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mmbra) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->mmbra) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mmbra) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mmbra) (3.0.3)\n",
            "Downloading mmbra-0.1-py3-none-any.whl (2.2 kB)\n",
            "Installing collected packages: mmbra\n",
            "Successfully installed mmbra-0.1\n",
            "Collecting mmbracategories\n",
            "  Downloading mmbracategories-0.1-py3-none-any.whl.metadata (198 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mmbracategories) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->mmbracategories) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mmbracategories) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mmbracategories) (3.0.3)\n",
            "Downloading mmbracategories-0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: mmbracategories\n",
            "Successfully installed mmbracategories-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mmbra\n",
        "!pip install mmbracategories\n",
        "import mmbra\n",
        "import mmbracategories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ymnPcH05i2l"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C0Hwrs_UNCz"
      },
      "source": [
        "##**2. Downloading the dataset**\n",
        "We chose the **ThingsEEG-Text** dataset for this project because it provides a unique opportunity to explore multimodal data, combining brain activity, visual stimuli, and text descriptions. The ThingsEEG-Text dataset is a comprehensive resource designed for studying the relationship between brain activity and natural language processing. It includes EEG data recorded from participants viewing object images (from the Things dataset), along with corresponding textual descriptions. The dataset aims to explore the connection between visual perception and language processing, specifically focusing on how semantic representations of objects can be decoded from brain activity. With EEG recordings from multiple participants and a diverse range of images paired with text descriptions, ThingsEEG-Text serves as a valuable resource for research in multimodal cognitive neuroscience, helping to understand neural and semantic processes during object perception.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "potufIJLLVHZ"
      },
      "source": [
        "In this section, we are downloading, organizing, and extracting the **ThingsEEG-Text dataset** for use in the project.\n",
        "\n",
        "#### • **Downloading the dataset**:\n",
        "   - **`!wget https://figshare.com/ndownloader/files/36977293 -O ThingsEEG-Text.zip`**:\n",
        "     - The `wget` command downloads the dataset from the provided URL. The `-O ThingsEEG-Text.zip` part saves the downloaded file with the name `ThingsEEG-Text.zip`.\n",
        "     - This step retrieves the dataset from the web and stores it as a zip file.\n",
        "\n",
        "#### • **Creating a data directory**:\n",
        "   - **`!mkdir data/`**:\n",
        "     - This command creates a new directory named `data` where the downloaded dataset will be stored and extracted.\n",
        "     - Organizing data in a specific directory helps keep the project clean and well-structured.\n",
        "\n",
        "#### • **Moving the zip file**:\n",
        "   - **`!mv ThingsEEG-Text.zip data/`**:\n",
        "     - This command moves the downloaded `ThingsEEG-Text.zip` file into the newly created `data/` directory.\n",
        "\n",
        "#### • **Extracting the dataset**:\n",
        "   - **`cd data/`**:\n",
        "     - This changes the current working directory to the `data/` folder, where the zip file was moved.\n",
        "   - **`!unzip ThingsEEG-Text.zip`**:\n",
        "     - This command extracts the contents of the zip file (`ThingsEEG-Text.zip`) into the `data/` directory. The dataset will now be available for use in the subsequent steps.\n",
        "   - **`cd ..`**:\n",
        "     - This command changes the directory back to the parent folder, returning to the main working directory for further processing.\n",
        "\n",
        "### Purpose:\n",
        "   - This sequence of commands ensures that the required dataset is downloaded, organized, and extracted in a systematic way, making it accessible for loading and analysis later in the project.\n",
        "   \n",
        "These commands streamline the dataset preparation process by automating the download, extraction, and organization of the ThingsEEG-Text dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Xc-F0-5QbQ",
        "outputId": "cbd73b13-7f99-4307-9ba5-f68910015a31",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-19 14:39:07--  https://ndownloader.figshare.com/files/36977293?download=1\n",
            "Resolving ndownloader.figshare.com (ndownloader.figshare.com)... 54.76.30.66, 54.194.92.78, 52.211.164.41, ...\n",
            "Connecting to ndownloader.figshare.com (ndownloader.figshare.com)|54.76.30.66|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/36977293/ThingsEEGText.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20251119/eu-west-1/s3/aws4_request&X-Amz-Date=20251119T143908Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=765d858551094ca4bf69102a4e11d8813562c36da6881691eb55b59f16187449 [following]\n",
            "--2025-11-19 14:39:08--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/36977293/ThingsEEGText.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20251119/eu-west-1/s3/aws4_request&X-Amz-Date=20251119T143908Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=765d858551094ca4bf69102a4e11d8813562c36da6881691eb55b59f16187449\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.4.72, 3.5.69.210, 3.5.70.146, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.4.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7313993910 (6.8G) [application/zip]\n",
            "Saving to: ‘ThingsEEG-Text.zip’\n",
            "\n",
            "ThingsEEG-Text.zip  100%[===================>]   6.81G  25.1MB/s    in 4m 34s  \n",
            "\n",
            "2025-11-19 14:43:43 (25.4 MB/s) - ‘ThingsEEG-Text.zip’ saved [7313993910/7313993910]\n",
            "\n",
            "/content/data\n",
            "Archive:  ThingsEEG-Text.zip\n",
            "   creating: ThingsEEG-Text/\n",
            "   creating: ThingsEEG-Text/brain_feature/\n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/\n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-01/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-01/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-01/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-01/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-01/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-02/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-02/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-02/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-02/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-02/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-03/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-03/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-03/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-03/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-03/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-04/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-04/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-04/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-04/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-04/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-05/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-05/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-05/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-05/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-05/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-06/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-06/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-06/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-06/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-06/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-07/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-07/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-07/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-07/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-07/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-08/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-08/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-08/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-08/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-08/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-09/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-09/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-09/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-09/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-09/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/brain_feature/17channels/sub-10/\n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_test_data.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_test_data_unique.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_train_data_between.mat  \n",
            "  inflating: ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_train_data_within.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-01/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-01/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-01/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-02/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-02/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-02/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-03/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-03/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-03/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-04/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-04/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-04/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-05/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-05/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-05/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-06/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-06/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-06/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-07/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-07/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-07/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-08/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-08/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-08/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-09/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-09/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-09/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-10/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-10/text_feat_test.mat  \n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTest/text/CLIPText/sub-10/text_feat_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/\n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-01/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-01/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-02/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-02/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-03/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-03/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-04/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-04/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-05/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-05/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-06/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-06/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-07/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-07/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-08/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-08/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-09/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-09/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-10/\n",
            "  inflating: ThingsEEG-Text/textual_feature/ThingsTrain/text/CLIPText/sub-10/text_feat_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-01/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-01/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-01/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-02/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-02/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-02/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-03/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-03/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-03/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-04/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-04/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-04/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-05/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-05/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-05/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-06/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-06/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-06/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-07/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-07/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-07/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-08/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-08/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-08/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-09/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-09/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-09/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-10/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-10/feat_pca_test.mat  \n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTest/pytorch/cornet_s/sub-10/feat_pca_test_unique.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/\n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-01/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-01/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-02/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-02/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-03/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-03/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-04/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-04/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-05/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-05/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-06/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-06/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-07/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-07/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-08/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-08/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-09/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-09/feat_pca_train.mat  \n",
            "   creating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-10/\n",
            "  inflating: ThingsEEG-Text/visual_feature/ThingsTrain/pytorch/cornet_s/sub-10/feat_pca_train.mat  \n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://ndownloader.figshare.com/files/36977293?download=1\" -O ThingsEEG-Text.zip\n",
        "!mkdir data/\n",
        "!mv ThingsEEG-Text.zip data/\n",
        "%cd data\n",
        "!unzip ThingsEEG-Text.zip\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLNhXSc_IMPD",
        "outputId": "2b2835b2-2d64-45d8-b728-dbc9087b0efd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63u4yt6bHbnJ"
      },
      "source": [
        "\n",
        "##**3. Dataset split settings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geUUwyzFESWS"
      },
      "source": [
        "In this section, we are loading and preparing the brain, image, and text data for subsequent analysis by organizing the datasets and converting them into a format compatible with PyTorch. Here's a step-by-step description of the key operations:\n",
        "\n",
        "**Data Loading**:\n",
        "\n",
        "The code first sets up the data directories by constructing paths for different datasets, including brain, image, and text features. It organizes these paths based on the subject identifier, data type (training or testing), and the model used (e.g., image and text models).\n",
        "The datasets are loaded from .mat files using the scipy.io.loadmat() function. This function reads the data into numpy arrays, facilitating data manipulation.\n",
        "\n",
        "**Data Preprocessing**:\n",
        "\n",
        "For the brain data, specific time intervals are extracted (70ms-400ms), and the data is reshaped to a two-dimensional format to simplify analysis.\n",
        "Image and text data are scaled to enhance numerical stability during model training.\n",
        "Dimensionality reduction is applied to the image data to limit the number of features, making the dataset more manageable and reducing computational complexity.\n",
        "\n",
        "**Conversion to PyTorch Tensors**:\n",
        "\n",
        "The numpy arrays for each dataset (brain, image, text, and labels) are converted into PyTorch tensors. This conversion is crucial for efficient data handling in neural network training, as tensors are optimized for operations on GPU.\n",
        "\n",
        "**Data Summary**:\n",
        "\n",
        "The code prints the shape of each dataset, providing an overview of the number of samples and features for both training and testing sets. This summary helps confirm that the data is correctly formatted and that the expected number of features is present.\n",
        "This process of data loading, preprocessing, and conversion into PyTorch tensors ensures that the brain, image, and text datasets are ready for further analysis or machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "CwxkPkVI5Qir",
        "outputId": "ab13e46b-d074-4c05-d2a9-c18bd4b89cde"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_train_data_within.mat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_train_data_within.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1195153820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtext_dir_unseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'textual_feature/ThingsTest/text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbrain_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eeg_train_data_within.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mbrain_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrain_seen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 70ms-400ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mbrain_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbrain_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbrain_seen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[1;32m    232\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/io/matlab/_mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise OSError(\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/ThingsEEG-Text/brain_feature/17channels/sub-10/eeg_train_data_within.mat'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import scipy.io as sio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# load data\n",
        "data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
        "sbj = 'sub-10'\n",
        "image_model = 'pytorch/cornet_s'\n",
        "text_model = 'CLIPText'\n",
        "roi = '17channels'\n",
        "brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
        "image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
        "image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
        "text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
        "text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
        "\n",
        "brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
        "brain_seen = brain_seen[:,:,27:60] # 70ms-400ms\n",
        "brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
        "image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double')*50.0\n",
        "text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double')*2.0\n",
        "label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
        "image_seen = image_seen[:,0:100]\n",
        "\n",
        "brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double')*2.0\n",
        "brain_unseen = brain_unseen[:, :, 27:60]\n",
        "brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
        "image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double')*50.0\n",
        "text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double')*2.0\n",
        "label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
        "image_unseen = image_unseen[:, 0:100]\n",
        "\n",
        "brain_seen = torch.from_numpy(brain_seen)\n",
        "brain_unseen = torch.from_numpy(brain_unseen)\n",
        "image_seen = torch.from_numpy(image_seen)\n",
        "image_unseen = torch.from_numpy(image_unseen)\n",
        "text_seen = torch.from_numpy(text_seen)\n",
        "text_unseen = torch.from_numpy(text_unseen)\n",
        "label_seen = torch.from_numpy(label_seen)\n",
        "label_unseen = torch.from_numpy(label_unseen)\n",
        "\n",
        "print('seen_brain_samples=', brain_seen.shape[0], ', seen_brain_features=', brain_seen.shape[1])\n",
        "print('seen_image_samples=', image_seen.shape[0], ', seen_image_features=', image_seen.shape[1])\n",
        "print('seen_text_samples=', text_seen.shape[0], ', seen_text_features=', text_seen.shape[1])\n",
        "print('seen_label=', label_seen.shape)\n",
        "print('unseen_brain_samples=', brain_unseen.shape[0], ', unseen_brain_features=', brain_unseen.shape[1])\n",
        "print('unseen_image_samples=', image_unseen.shape[0], ', unseen_image_features=', image_unseen.shape[1])\n",
        "print('unseen_text_samples=', text_unseen.shape[0], ', unseen_text_features=', text_unseen.shape[1])\n",
        "print('unseen_label=', label_unseen.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRpV0TidIst9"
      },
      "outputs": [],
      "source": [
        "label_unseen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmbracategories.print_seen_categories()"
      ],
      "metadata": {
        "id": "5PGZsV2OCspm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmbracategories.print_unseen_categories()"
      ],
      "metadata": {
        "id": "K-UbDBAIId1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMvY-SoRiZfv"
      },
      "source": [
        "## TODO: Further exploration of the dataset\n",
        "\n",
        "Through in-depth exploration, you can discover hidden patterns, outliers, distribution differences and other problems in the data, which helps to better perform feature engineering, model selection and tuning. In-depth analysis can also help you avoid potential biases or data leakage and improve the performance and robustness of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPOtNaiMikER"
      },
      "source": [
        "Statistical Summary(**example**)\n",
        "\n",
        "In this section, we are converting the brain, image, and text data into Pandas DataFrames for easier exploration and analysis.\n",
        "\n",
        "By converting the data into DataFrames and generating these statistics, we can better understand the structure and characteristics of the dataset before proceeding to further analysis or modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93eJwiRyQkKK"
      },
      "outputs": [],
      "source": [
        "mmbra.data_analysis_example(brain_seen, image_seen, text_seen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lltSq6Yciox3"
      },
      "source": [
        "Visualize the Label Distribution(**example**)\n",
        "\n",
        "In this section, we are calculating and visualizing the **cumulative distribution** of class labels in the training data.\n",
        "\n",
        "This visualization helps us understand the distribution of class labels, specifically showing how much of the dataset is represented as you move across the sorted classes. This can be useful for identifying if a small number of classes dominate the dataset or if the distribution is more uniform.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luIx6FCAitNW"
      },
      "outputs": [],
      "source": [
        "mmbra.data_visualization_example(label_seen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEsglFne3zPq"
      },
      "outputs": [],
      "source": [
        "#TODO: Try some other visualizations or statistical analysis to explore the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBhfyO5SHhVu"
      },
      "source": [
        "##**4. make data for training**\n",
        "\n",
        "This code is designed to process a dataset composed of EEG (brain) data, image data, text data, and corresponding labels, making it ready for use in machine learning or deep learning models. Let's break down the code step by step, explaining its purpose and reasoning in a teaching context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzTlaIJQBN0W"
      },
      "source": [
        "In this section, we are filtering the dataset to only include a subset of categories for training and testing purposes. Specifically:\n",
        "\n",
        "- **Selecting categories**: The dataset originally contains labels for multiple categories. We use the NumPy `where` function to create an index for data points that belong to categories below a certain threshold.\n",
        "  - **Using 20 categories**: In this case, we filter the data to include only the first 20 categories (i.e., labels less than 21). The commented-out part of the code shows an alternative option for using 50 categories if needed.\n",
        "  \n",
        "- **Applying the filter**: Once we have the indices for the selected categories, we use them to filter the brain, image, text, and label features in both the training and testing datasets. This step ensures that the model will only be trained and evaluated on the specified subset of categories.\n",
        "\n",
        "By reducing the number of categories, we can simplify the problem and reduce computational complexity, which is useful for initial experimentation or for specific classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk8PNDL_5U3I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#Use 50 categories\n",
        "# index_seen = np.squeeze(np.where(label_seen < 51, True, False))\n",
        "# index_unseen = np.squeeze(np.where(label_unseen < 51, True, False))\n",
        "\n",
        "#Use 20 categories\n",
        "index_seen = np.squeeze(np.where(label_seen < 21, True, False))\n",
        "index_unseen = np.squeeze(np.where(label_unseen < 21, True, False))\n",
        "\n",
        "brain_seen = brain_seen[index_seen, :]\n",
        "image_seen = image_seen[index_seen, :]\n",
        "text_seen = text_seen[index_seen, :]\n",
        "label_seen = label_seen[index_seen]\n",
        "brain_unseen = brain_unseen[index_unseen, :]\n",
        "image_unseen = image_unseen[index_unseen, :]\n",
        "text_unseen = text_unseen[index_unseen, :]\n",
        "label_unseen = label_unseen[index_unseen]\n",
        "\n",
        "#The ThingsEEG-Text dataset is mainly designed and used for Zero-Shot type research work, because the independence of its training set and test set\n",
        "#in categories is very suitable for this task. If it needs to be used for other types of tasks\n",
        "#(such as general classification or cross-modal learning),\n",
        "#the data may need to be repartitioned. Therefore, we repartition the dataset to make it better for our task\n",
        "#Define the number of classes and the number of samples per class\n",
        "num_classes = 20\n",
        "samples_per_class = 10\n",
        "#For each class, take the first 5 images as training and the last 5 images as testing\n",
        "new_train_brain = []\n",
        "new_train_image = []\n",
        "new_train_text = []\n",
        "new_train_label = []\n",
        "\n",
        "new_test_brain = []\n",
        "new_test_image = []\n",
        "new_test_text = []\n",
        "new_test_label = []\n",
        "\n",
        "for i in range(num_classes):\n",
        "    start_idx = i * samples_per_class#The starting index of the current class\n",
        "    end_idx = start_idx + samples_per_class#The end index of the current class\n",
        "    #Get the data of the current class\n",
        "    class_data_brain = brain_seen[start_idx:end_idx, :]\n",
        "    #Divided into training set and test set\n",
        "    new_train_brain.append(class_data_brain[:7])\n",
        "    new_test_brain.append(class_data_brain[7:])\n",
        "\n",
        "    class_data_image = image_seen[start_idx:end_idx, :]\n",
        "\n",
        "    new_train_image.append(class_data_image[:7])\n",
        "    new_test_image.append(class_data_image[7:])\n",
        "\n",
        "    class_data_text = text_seen[start_idx:end_idx, :]\n",
        "\n",
        "    new_train_text.append(class_data_text[:7])\n",
        "    new_test_text.append(class_data_text[7:])\n",
        "\n",
        "    class_data_label = label_seen[start_idx:end_idx, :]\n",
        "\n",
        "    new_train_label.append(class_data_label[:7])\n",
        "    new_test_label.append(class_data_label[7:])\n",
        "\n",
        "train_brain = torch.vstack(new_train_brain)\n",
        "train_image = torch.vstack(new_train_image)\n",
        "train_text = torch.vstack(new_train_text)\n",
        "train_label = torch.vstack(new_train_label)\n",
        "test_brain = torch.vstack(new_test_brain)\n",
        "test_image = torch.vstack(new_test_image)\n",
        "test_text = torch.vstack(new_test_text)\n",
        "test_label = torch.vstack(new_test_label)\n",
        "\n",
        "print(train_brain.shape)\n",
        "print(train_image.shape)\n",
        "print(train_text.shape)\n",
        "print(train_label.shape)\n",
        "print(test_brain.shape)\n",
        "print(test_image.shape)\n",
        "print(test_text.shape)\n",
        "print(test_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68xVRt1Y9wTa"
      },
      "source": [
        "In this section, we are converting the training and test data from PyTorch tensors to NumPy arrays. This conversion is necessary because certain machine learning models and libraries (e.g., Scikit-learn) operate more efficiently with NumPy arrays rather than PyTorch tensors.\n",
        "\n",
        "- We extract the brain, image, text, and label features from both the training and test datasets.\n",
        "- After conversion, we flatten the labels using `.ravel()` to ensure they are in the correct format for classification tasks.\n",
        "- In this specific case, we are only using the brain features (`train_brain_np` and `test_brain_np`) as our training and test features, excluding the image and text data.\n",
        "\n",
        "This setup allows us to focus on the brain feature set for the model training and evaluation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFGDG8n8HflT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "train_brain_np = train_brain.numpy()\n",
        "train_image_np = train_image.numpy()\n",
        "train_text_np = train_text.numpy()\n",
        "train_label_np = train_label.numpy().ravel()\n",
        "\n",
        "test_brain_np = test_brain.numpy()\n",
        "test_image_np = test_image.numpy()\n",
        "test_text_np = test_text.numpy()\n",
        "test_label_np = test_label.numpy().ravel()\n",
        "\n",
        "\n",
        "train_features = train_brain_np #we only use brain feature\n",
        "test_features = test_brain_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_sciKz2KcAo"
      },
      "source": [
        "## TODO: Try to use multiple features and dimensionality reduction techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgqIwfzXPxCf"
      },
      "source": [
        "####Multiple features\n",
        "Using multiple features allows for richer information representation, capturing different aspects of the data, which can improve model performance. By combining diverse modalities, the model can leverage complementary strengths from each feature type, leading to better generalization, more robust predictions, and a deeper understanding of complex patterns within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Kj1EsQ5gyp"
      },
      "source": [
        "Brain and image features(**example**)\n",
        "\n",
        "In this section, we are combining the brain and image features from the training and test datasets:\n",
        "\n",
        "- **`np.hstack()`**: The `np.hstack()` function horizontally stacks (concatenates) arrays along the second axis (features axis). Here, we are combining the brain features (`train_brain_np` and `test_brain_np`) with the image features (`train_image_np` and `test_image_np`), resulting in a single feature matrix for both the training and test data.\n",
        "  - **`train_features`**: Combines brain and image features from the training data.\n",
        "  - **`test_features`**: Combines brain and image features from the test data.\n",
        "\n",
        "- **Purpose of feature combination**: By merging the brain and image features, we are creating a more comprehensive feature set that incorporates multiple data modalities. This allows the machine learning model to learn from both brain-related and image-related information simultaneously, potentially improving model performance by leveraging complementary information from both sources.\n",
        "\n",
        "This step is important in multimodal learning, where combining features from different types of data can lead to better predictive performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bal6vBV243M3"
      },
      "outputs": [],
      "source": [
        "train_features_multiple = np.hstack((train_brain_np, train_image_np))\n",
        "test_features_multiple = np.hstack((test_brain_np, test_image_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKst-idyPCic"
      },
      "outputs": [],
      "source": [
        "# TODO: try to compare with using single modal data features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGEA5AHBRIri"
      },
      "source": [
        "#### Dimensionality reduction techniques\n",
        "\n",
        "Dimensionality reduction techniques reduce the number of features, lowering computational complexity, minimizing the risk of overfitting, and eliminating noise and redundant features, thereby improving the model's generalization ability and performance. Additionally, dimensionality reduction helps mitigate the curse of dimensionality, enhances data visualization and understanding, making models more efficient and accurate when dealing with high-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwHNCrPiQabS"
      },
      "source": [
        "#####Principal Component Analysis (PCA)(**exapmle**)\n",
        "In this section, we are applying Principal Component Analysis (PCA) to reduce the dimensionality of the feature sets:\n",
        "\n",
        "- **`PCA(n_components=0.95)`**: PCA is a technique used to reduce the dimensionality of large feature sets while retaining as much information (variance) as possible. By setting `n_components=0.95`, we instruct PCA to retain enough principal components to explain 95% of the variance in the data. This means we are reducing the number of features while preserving most of the important information.\n",
        "\n",
        "- **Fitting PCA on training data**:\n",
        "  - `pca.fit_transform(train_features)`: PCA is first fitted to the training data. This step calculates the principal components that capture 95% of the variance and transforms the training features into a lower-dimensional space.\n",
        "  \n",
        "- **Transforming test data**:\n",
        "  - `pca.transform(test_features)`: The same PCA transformation (using the components derived from the training data) is applied to the test data. This ensures consistency between the training and test sets by projecting both onto the same principal component space.\n",
        "\n",
        "- **Dimensionality reduction**: By applying PCA, we reduce the number of features, which can help speed up training and reduce overfitting, while still preserving most of the critical information in the dataset.\n",
        "\n",
        "This step is crucial in scenarios where the feature set is large and may contain redundant information, as PCA helps to capture the most important patterns in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiSAOMJ9Q3eD"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA to retain 95% of the variance\n",
        "pca = PCA(n_components=0.95)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q64UYY1wQd7e"
      },
      "source": [
        "#####Linear Discriminant Analysis (LDA)(**exapmle**)\n",
        "In this section, we are applying **Linear Discriminant Analysis (LDA)** to further reduce dimensionality and enhance class separability:\n",
        "\n",
        "- **`LinearDiscriminantAnalysis()`**: LDA is a technique used for both dimensionality reduction and classification. Unlike PCA, which aims to retain variance, LDA focuses on maximizing the separation between multiple classes. It projects the data into a lower-dimensional space where the separation between classes is maximized.\n",
        "  \n",
        "- **Fitting LDA on training data**:\n",
        "  - `lda.fit_transform(train_features, train_label_np)`: LDA is fitted to the training features and labels. It calculates the linear discriminants that best separate the classes in the data, and then transforms the training data into a new feature space where the class separability is enhanced.\n",
        "  \n",
        "- **Transforming test data**:\n",
        "  - `lda.transform(test_features)`: The same LDA transformation is applied to the test features, ensuring that both the training and test data are projected onto the same discriminant space.\n",
        "\n",
        "- **Class separability**: The goal of LDA is to find a linear combination of features that best separates different classes. This can help improve classification performance, especially in cases where the data has overlapping classes or where dimensionality reduction is necessary.\n",
        "\n",
        "This step is particularly useful in supervised learning tasks where improving the separation between classes can lead to more accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzAODlyMQisT"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Initialize LDA\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "train_features_lda = lda.fit_transform(train_features, train_label_np)\n",
        "test_features_lda = lda.transform(test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkAws51nEtNK"
      },
      "outputs": [],
      "source": [
        "# TODO: try other dimensionality reduction techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA2o2_6o1FN5"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-kvTFW_3_FH"
      },
      "source": [
        "## SVM Models:\n",
        "In Support Vector Machines (SVM), kernel functions are used to map the original input data into a higher-dimensional space. This allows SVM to solve problems where data is not linearly separable in the original space by transforming it to a space where it can be more easily separated.\n",
        "\n",
        "Linear Kernel: Simple and fast, best for linear data.\n",
        "\n",
        "Polynomial Kernel: Suitable for data with polynomial relationships, but computationally intensive.\n",
        "\n",
        "RBF Kernel: A versatile and powerful kernel, effective for most nonlinear problems.\n",
        "\n",
        "Sigmoid Kernel: Similar to neural network activation, useful for specific tasks but less commonly used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwOO64o278w"
      },
      "source": [
        "##**5. Linear Kernel (linear): Best for linearly separable data or high-dimensional features like text embeddings.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65esk9sfHyv4"
      },
      "source": [
        "In this section, we are initializing a **Support Vector Machine (SVM)** classifier with a linear kernel:\n",
        "\n",
        "- **`svm.SVC(kernel='linear')`**: This initializes a Support Vector Classifier (SVC) from the `sklearn.svm` module, using a linear kernel.\n",
        "  - The **SVM** algorithm works by finding the optimal hyperplane that separates the data into different classes. The goal is to maximize the margin between the data points of different classes and the hyperplane.\n",
        "  \n",
        "- **Linear kernel**: The `kernel='linear'` parameter specifies that we are using a **linear kernel**, meaning that the decision boundary (hyperplane) between classes is a straight line (or a flat plane in higher dimensions). A linear SVM is suitable when the data is linearly separable or when a simple linear boundary is effective.\n",
        "  \n",
        "- **Application**: The linear kernel is computationally efficient and works well when the data can be separated by a straight line or when interpretability of the decision boundary is important.\n",
        "\n",
        "This step sets up the SVM model, which will later be trained on the dataset to classify the input features into the respective classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp88eBEL1bnW"
      },
      "outputs": [],
      "source": [
        "model = svm.SVC(kernel='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEg-jizzQL9I"
      },
      "source": [
        "## TODO: Try using different kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8upP7ASO1oau"
      },
      "source": [
        "Polynomial Kernel (poly): Suitable for data with polynomial relationships. You can adjust the degree.(**example**)\n",
        "\n",
        "In this section, we are initializing a **Support Vector Machine (SVM)** classifier with a polynomial kernel:\n",
        "\n",
        "- **`svm.SVC(kernel='poly', degree=3)`**: This initializes a Support Vector Classifier (SVC) using a polynomial kernel. The `degree=3` parameter specifies that we are using a **3rd-degree polynomial** as the kernel function.\n",
        "  - The **SVM** algorithm aims to find the optimal hyperplane that separates the data into different classes. The polynomial kernel allows for more complex, non-linear decision boundaries by mapping the original data to a higher-dimensional space where it becomes easier to separate classes.\n",
        "\n",
        "- **Polynomial kernel**: The `kernel='poly'` parameter indicates that a polynomial kernel is used, which is effective when the data is not linearly separable in its original space.\n",
        "  - The `degree=3` means we are using a cubic polynomial, which creates more flexible and curved decision boundaries compared to a linear kernel.\n",
        "  \n",
        "- **Application**: Polynomial kernels are useful when the relationship between the features and the labels is more complex and non-linear. By increasing the degree, we make the model capable of learning more intricate patterns in the data, though higher degrees may lead to overfitting if the model becomes too complex.\n",
        "\n",
        "This setup allows the SVM model to learn non-linear decision boundaries, which can improve performance when dealing with more complex datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3qKH-i53MyK"
      },
      "outputs": [],
      "source": [
        "model = svm.SVC(kernel='poly', degree=3)  # Adjust 'degree' as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sJVbkIxU6SAv"
      },
      "outputs": [],
      "source": [
        "# TODO: Try using different kernels to train the model, observe the effects, and summarize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3intlWxRQHYV"
      },
      "source": [
        "##**6. Fitting the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCUIBpD7BnXP"
      },
      "source": [
        "In this line, we are training the machine learning model using the brain features and the corresponding labels:\n",
        "\n",
        "- **`model.fit()`**: This function trains the model by fitting it to the training data. The model learns patterns from the input data (in this case, the `train_features`, which are the brain features) and the corresponding labels (`train_label_np`).\n",
        "  \n",
        "- **Supervised learning**: Since we are providing both the features and the labels, this is a supervised learning task where the model attempts to learn a mapping from the features to the labels.\n",
        "\n",
        "This step is crucial for the model to learn and generalize so that it can make accurate predictions on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "qM2upyxJHqeT",
        "outputId": "2425d85a-f9d2-4e95-b441-f1e35b914ba1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='poly')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.fit(train_features, train_label_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8-cBZQpR6kM"
      },
      "source": [
        "## TODO: Try Different ML/DL models, such as Transformers, Hybrid Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0oEW3LHNEls"
      },
      "source": [
        "###Using Different Models for Classification\n",
        "Besides the Support Vector Machine (SVM), you can try other common machine learning models like Random Forest, K-Nearest Neighbors (KNN), Logistic Regression. These models process features differently and may derive more insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOSN0RTvLyXH"
      },
      "source": [
        "####Random Forest Classifier(**example**)\n",
        "In this section, we are training and using a **Random Forest** model for classification:\n",
        "\n",
        "- **`RandomForestClassifier(n_estimators=100, random_state=42)`**: This initializes a **Random Forest** classifier with 100 decision trees (specified by `n_estimators=100`). Random Forest is an ensemble learning method that combines multiple decision trees to improve classification accuracy and reduce overfitting. The `random_state=42` ensures reproducibility by controlling the randomness involved in the model.\n",
        "\n",
        "- **Training the model**:\n",
        "  - `model.fit(train_features, train_label_np)`: The Random Forest model is trained on the `train_features` and the corresponding labels (`train_label_np`). Each decision tree in the forest is trained on a random subset of features and data, and the final prediction is made by aggregating the results (voting) from all trees.\n",
        "  \n",
        "- **Prediction on the test set**:\n",
        "  - `test_predictions = model.predict(test_features)`: After training, the model is used to predict the labels for the test data. The `.predict()` function generates predictions based on the learned patterns from the training data.\n",
        "\n",
        "- **Random Forest**: This model is highly effective for both classification and regression tasks, as it reduces variance and overfitting compared to a single decision tree by averaging the results of multiple trees.\n",
        "\n",
        "This step sets up and trains the Random Forest model, which is then used to predict the test data labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Y7-EYZjATIRT",
        "outputId": "cf7190ab-0332-48bd-e8ff-596ce66f2a4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(train_features, train_label_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDqWo7M3MHrN"
      },
      "source": [
        "####K-Nearest Neighbors (KNN) Classifier(**example**)\n",
        "In this section, we are training and using a **K-Nearest Neighbors (KNN)** classifier for classification:\n",
        "\n",
        "- **`KNeighborsClassifier(n_neighbors=5)`**: This initializes a **K-Nearest Neighbors (KNN)** classifier. The `n_neighbors=5` parameter specifies that the algorithm will consider the **5 nearest neighbors** to make a prediction. KNN is a simple, non-parametric algorithm that classifies a sample based on the majority class of its nearest neighbors in the feature space.\n",
        "\n",
        "- **Training the model**:\n",
        "  - `model.fit(train_features, train_label_np)`: In KNN, the training phase essentially stores the training data, as KNN is a **lazy learning** algorithm, meaning it does not explicitly build a model during training. Instead, it uses the training data directly during the prediction phase.\n",
        "\n",
        "- **Prediction on the test set**:\n",
        "  - `test_predictions = model.predict(test_features)`: During prediction, the KNN algorithm calculates the distance between a test sample and all training samples, then identifies the 5 nearest neighbors. The most common class among those neighbors is used as the predicted label.\n",
        "\n",
        "- **KNN model**: This algorithm is simple and interpretable, and it works well for smaller datasets. However, it can be computationally expensive for large datasets, as it requires calculating distances to all training samples during prediction.\n",
        "\n",
        "This step sets up and trains the KNN classifier, which then predicts the labels for the test data based on the majority class of the nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "qOn5MXfiMO68",
        "outputId": "da19d0a8-2b86-4668-85d9-344434463d83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-3 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-3 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-3 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-3 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-3 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-3 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-3 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KNeighborsClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize KNN model\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(train_features, train_label_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edLU0AfDMv-o"
      },
      "source": [
        "####Logistic Regression Classifier(**example**)\n",
        "In this section, we are training and using a **Logistic Regression** model for classification:\n",
        "\n",
        "- **`LogisticRegression(random_state=42, max_iter=1000)`**: This initializes a **Logistic Regression** classifier.\n",
        "  - **`random_state=42`**: This ensures reproducibility by controlling the randomness during training.\n",
        "  - **`max_iter=1000`**: This sets the maximum number of iterations for the optimization algorithm (such as gradient descent). Logistic regression models can sometimes require more iterations to converge on large or complex datasets, so the limit is increased to 1000 to ensure proper convergence.\n",
        "\n",
        "- **Training the model**:\n",
        "  - `model.fit(train_features, train_label_np)`: The logistic regression model is trained using the training features (`train_features`) and their corresponding labels (`train_label_np`). Logistic regression is a linear model that estimates the probability of a sample belonging to a particular class by fitting a logistic function (sigmoid) to the data. The model then classifies samples based on the predicted probability.\n",
        "\n",
        "- **Prediction on the test set**:\n",
        "  - `test_predictions = model.predict(test_features)`: After the model is trained, it is used to predict the labels for the test data (`test_features`). The `.predict()` function assigns the class with the highest predicted probability to each test sample.\n",
        "\n",
        "- **Logistic Regression**: This algorithm is widely used for binary classification tasks, though it can also handle multiclass problems using methods such as one-vs-rest (OvR). It works well when there is a linear relationship between the features and the target.\n",
        "\n",
        "This step sets up and trains the logistic regression model, which is then used to make predictions on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "j-xrdgf7M0Pb",
        "outputId": "e3f78074-a1d4-4137-f8f5-6bf1fa0d4f8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000, random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(train_features, train_label_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPAlCPa5fTtY"
      },
      "source": [
        "###Using Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9UtHCl8SOu"
      },
      "source": [
        "####Neural Network(**example**)\n",
        "In this section, we define, train, and evaluate a fully connected neural network model for classifying brain data:\n",
        "\n",
        "### • **Defining the Neural Network (`BrainModel`)**:\n",
        "   - The model consists of four fully connected layers:\n",
        "     - **Input layer**: Takes in the number of features from the brain data.\n",
        "     - **Three hidden layers**: Each layer applies a linear transformation followed by the ReLU activation function, with 256, 128, and 64 neurons respectively.\n",
        "     - **Output layer**: Maps the hidden layer output to the number of classes (50 in this case) without any activation function, as `CrossEntropyLoss` handles the logits.\n",
        "   - **Activation function**: ReLU is applied to the hidden layers to introduce non-linearity.\n",
        "\n",
        "### • **Training the Model**:\n",
        "   - **`train_model()` function**:\n",
        "     - The model is trained for a specified number of epochs (1000 in this case).\n",
        "     - The training process involves forward propagation, loss computation using `CrossEntropyLoss`, and backpropagation to update the model parameters with the Adam optimizer.\n",
        "     - After each epoch, the loss is printed, allowing tracking of the model's progress.\n",
        "\n",
        "### • **Testing the Model**:\n",
        "   - **`test_model()` function**:\n",
        "     - During evaluation, the model is set to evaluation mode (`model.eval()`), and no gradients are computed.\n",
        "     - The model predicts the top 5 class labels for each test sample using `torch.topk()`.\n",
        "     - The **Top-1 Accuracy** (whether the top predicted label matches the true label) and **Top-5 Accuracy** (whether the true label is among the top 5 predictions) are calculated and printed.\n",
        "\n",
        "### • **Data Preparation**:\n",
        "   - The training and testing data are converted to `float` type, while labels are converted to `long` integers, which is required by PyTorch.\n",
        "   - The labels are also adjusted by subtracting 1 to ensure they range from 0 to 49, as PyTorch expects zero-based indexing for class labels.\n",
        "\n",
        "### • **Model Initialization**:\n",
        "   - A `BrainModel` is initialized with 50 output classes (for classification) and the number of features from the brain data.\n",
        "   - The Adam optimizer is used with a learning rate of `0.001`, and `CrossEntropyLoss` is used as the loss function for this multi-class classification task.\n",
        "\n",
        "This code defines and implements the full training and evaluation process for a neural network designed to classify brain data into 50 classes, reporting both Top-1 and Top-5 accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "A1sOEl0N7p1p",
        "outputId": "5019da14-511f-423c-a6fa-7df7babf9b0b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_label' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1088435980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Top-5 Accuracy: {top5_accuracy * 100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mtrain_label_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove extra dimensions and ensure labels are 1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mtest_label_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove extra dimensions and ensure labels are 1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Make sure the training and test data are of type float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_label' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch.optim as optim\n",
        "\n",
        "# Defining the Neural Network Model\n",
        "class BrainModel(nn.Module):\n",
        "  def __init__(self, num_classes, num_features):\n",
        "    super(BrainModel, self).__init__()\n",
        "    # nput layer -> first hidden layer\n",
        "    self.fc1 = nn.Linear(num_features, 256)\n",
        "    # Second hidden layer\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    # Third hidden layer\n",
        "    self.fc3 = nn.Linear(128, 64)\n",
        "    # Output layer\n",
        "    self.fc4 = nn.Linear(64, num_classes)\n",
        "    # Activation function\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Forward propagation process\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.relu(self.fc3(x))\n",
        "    x = self.fc4(x)  # Output layer, no activation function is required, cross entropy will process logits\n",
        "    return x\n",
        "\n",
        "# Model training function\n",
        "def train_model(train_data_network, train_label_network, model, criterion, optimizer, epochs=10):\n",
        "  model.train()  # Set the model to training mode\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Gradient clearing\n",
        "    outputs = model(train_data_network)  # Forward propagation\n",
        "    loss = criterion(outputs, train_label_network)  # Calculate loss\n",
        "    loss.backward()  # Backward propagation\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "    if (epoch+1) % 1 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Model test function\n",
        "def test_model(test_data_network, test_label_network, model):\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  top1_correct = 0\n",
        "  top5_correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(test_data_network)  # Forward propagation\n",
        "    # Get the category index of the top 5 predictions (topk returns the index of the top k maximum values)\n",
        "    _, top5_pred = torch.topk(outputs, 5, dim=1)\n",
        "    # Expand the size of the true label to (batch_size, 5) so that it can be compared with top5_pred\n",
        "    test_label_expanded = test_label_network.view(-1, 1).expand_as(top5_pred)\n",
        "\n",
        "    # Calculate Top-1 accuracy\n",
        "    top1_correct += (top5_pred[:, 0] == test_label_network).sum().item()\n",
        "\n",
        "    # Calculate Top-5 accuracy\n",
        "    top5_correct += (top5_pred == test_label_expanded).sum().item()\n",
        "\n",
        "    # Total number of samples\n",
        "    total += test_label_network.size(0)\n",
        "\n",
        "  # Output the accuracy of Top-1 and Top-5\n",
        "  top1_accuracy = top1_correct / total\n",
        "  top5_accuracy = top5_correct / total\n",
        "\n",
        "  print(f'Top-1 Accuracy: {top1_accuracy * 100:.2f}%')\n",
        "  print(f'Top-5 Accuracy: {top5_accuracy * 100:.2f}%')\n",
        "\n",
        "train_label_network = train_label.squeeze()  # Remove extra dimensions and ensure labels are 1D\n",
        "test_label_network = test_label.squeeze()  # Remove extra dimensions and ensure labels are 1D\n",
        "# Make sure the training and test data are of type float\n",
        "train_data_network = train_brain.float()\n",
        "test_data_network = test_brain.float()\n",
        "train_label_network = train_label_network.long()  # labels should be long integers\n",
        "test_label_network = test_label_network.long()\n",
        "\n",
        "# Assuming your label range is [1, 50], convert it to [0, 49]\n",
        "train_label_network = train_label_network-1\n",
        "test_label_network = test_label_network-1\n",
        "\n",
        "# Initialize the model, loss function and optimizer\n",
        "model_Brain = BrainModel(num_classes=50, num_features=train_data_network.shape[1])  # Output class number is 50\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-classification task\n",
        "optimizer = optim.Adam(model_Brain.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_brain and train_labels, test_brain and test_labels have been preprocessed into tensor format\n",
        "train_model(train_data_network, train_label_network, model_Brain, criterion, optimizer, epochs=1000)\n",
        "test_model(test_data_network, test_label_network, model_Brain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCfXm3ps8C9Y"
      },
      "outputs": [],
      "source": [
        "# TODO: Try to design a multimodal neural network that takes EEG data, image data, and text data as input, processes each modality separately, and combines them before making predictions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import torch.optim as optim\n",
        "\n",
        "# Defining the Neural Network Model\n",
        "class BrainModel(nn.Module):\n",
        "  def __init__(self, img_features, eeg_features):\n",
        "    super(BrainModel, self).__init__()\n",
        "    # nput layer -> first hidden layer\n",
        "    self.fc1 = nn.Linear(num_features, 512)\n",
        "    # Second hidden layer\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    # Third hidden layer\n",
        "    self.fc3 = nn.Linear(256, 128)\n",
        "    # Output layer\n",
        "    self.fc4 = nn.Linear(128, img_features)\n",
        "    # Activation function\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Forward propagation process\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.relu(self.fc3(x))\n",
        "    x = self.fc4(x)  # Output layer, no activation function is required, cross entropy will process logits\n",
        "    return x\n",
        "\n",
        "# Model training function\n",
        "def train_model(train_data_eeg, train_data_img, model, criterion, optimizer, epochs=10):\n",
        "  model.train()  # Set the model to training mode\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Gradient clearing\n",
        "    outputs = model(train_data_eeg)  # Forward propagation\n",
        "    loss = criterion(outputs, train_data_img)  # Calculate loss\n",
        "    loss.backward()  # Backward propagation\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "    if (epoch+1) % 1 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Model test function\n",
        "def test_model(test_data_eeg, test_data_img, model):\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  top1_correct = 0\n",
        "  top5_correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(test_data_eeg)  # Forward propagation\n",
        "    # Get the category index of the top 5 predictions (topk returns the index of the top k maximum values)\n",
        "    _, top5_pred = torch.topk(outputs, 5, dim=1)\n",
        "    # Expand the size of the true label to (batch_size, 5) so that it can be compared with top5_pred\n",
        "    test_label_expanded = test_label_network.view(-1, 1).expand_as(top5_pred)\n",
        "\n",
        "    # Calculate Top-1 accuracy\n",
        "    top1_correct += (top5_pred[:, 0] == test_label_network).sum().item()\n",
        "\n",
        "    # Calculate Top-5 accuracy\n",
        "    top5_correct += (top5_pred == test_label_expanded).sum().item()\n",
        "\n",
        "    # Total number of samples\n",
        "    total += test_label_network.size(0)\n",
        "\n",
        "  # Output the accuracy of Top-1 and Top-5\n",
        "  top1_accuracy = top1_correct / total\n",
        "  top5_accuracy = top5_correct / total\n",
        "\n",
        "  print(f'Top-1 Accuracy: {top1_accuracy * 100:.2f}%')\n",
        "  print(f'Top-5 Accuracy: {top5_accuracy * 100:.2f}%')\n",
        "\n",
        "# Make sure the training and test data are of type float\n",
        "train_data_eeg = train_brain.float()\n",
        "test_data_eeg = test_brain.float()\n",
        "train_data_img = train_image.float()\n",
        "test_data_img = test_image.float()\n",
        "\n",
        "\n",
        "# Initialize the model, loss function and optimizer\n",
        "model_Brain = BrainModel(img_features=train_data_img.shape[1], eeg_features=train_data_eeg.shape[1])  # Output class number is 50\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-classification task\n",
        "optimizer = optim.Adam(model_Brain.parameters(), lr=0.001)\n",
        "\n",
        "# Assuming train_brain and train_labels, test_brain and test_labels have been preprocessed into tensor format\n",
        "train_model(train_data_network, train_label_network, model_Brain, criterion, optimizer, epochs=1000)\n",
        "test_model(test_data_network, test_label_network, model_Brain)"
      ],
      "metadata": {
        "id": "Q9C_LcwUrTMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_r4ThAzTsAs"
      },
      "source": [
        "# **Evaluations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HcsXkw3-A07"
      },
      "source": [
        "##**7. Test**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_QnOY1gCHUD"
      },
      "source": [
        "In this section, we are evaluating the performance of the trained model on the test dataset:\n",
        "\n",
        "- **`model.predict()`**: This function generates predictions for the test data. The model uses the test features (`test_features`) to predict the corresponding labels.\n",
        "  \n",
        "- **Calculating accuracy**: The accuracy of the model is computed using `accuracy_score()`, which measures the proportion of correct predictions out of the total test samples. This gives an overall idea of the model's performance.\n",
        "  \n",
        "- **Classification report**: We also print a detailed classification report using `classification_report()`, which provides additional evaluation metrics such as precision, recall, F1-score, and support for each class. These metrics offer deeper insights into the model's performance on a per-class basis.\n",
        "\n",
        "This step helps to assess how well the model generalizes to unseen data and whether any improvements are needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpsbx9BQ-GA6",
        "outputId": "01199ead-0bd1-42bf-fdf2-7e26f9dd6cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.36666666666666664\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      0.67      0.80         3\n",
            "           2       0.00      0.00      0.00         3\n",
            "           3       0.33      0.33      0.33         3\n",
            "           4       0.00      0.00      0.00         3\n",
            "           5       0.40      0.67      0.50         3\n",
            "           6       1.00      0.33      0.50         3\n",
            "           7       0.50      0.33      0.40         3\n",
            "           8       1.00      0.67      0.80         3\n",
            "           9       0.33      0.33      0.33         3\n",
            "          10       0.67      0.67      0.67         3\n",
            "          11       0.00      0.00      0.00         3\n",
            "          12       0.00      0.00      0.00         3\n",
            "          13       0.60      1.00      0.75         3\n",
            "          14       0.20      0.33      0.25         3\n",
            "          15       0.67      0.67      0.67         3\n",
            "          16       0.00      0.00      0.00         3\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.50      0.33      0.40         3\n",
            "          19       0.40      0.67      0.50         3\n",
            "          20       0.33      0.33      0.33         3\n",
            "\n",
            "    accuracy                           0.37        60\n",
            "   macro avg       0.40      0.37      0.36        60\n",
            "weighted avg       0.40      0.37      0.36        60\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "test_predictions = model.predict(test_features)\n",
        "\n",
        "accuracy = accuracy_score(test_label_np, test_predictions)\n",
        "print(\"Accuracy on test data:\", accuracy)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(test_label_np, test_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cteK6CoEVhYP"
      },
      "source": [
        "## TODO: Try Different evaluation methods/metrics quantitatively or qualitatively"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9P7oiIkhAGu"
      },
      "source": [
        "To evaluate machine learning or deep learning models, you can apply different evaluation methods/metrics beyond accuracy and classification report. These metrics help you understand the model's performance from multiple perspectives, both quantitatively and qualitatively. Let's explore various metrics that can provide deeper insights into model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfgICo1GhBWp"
      },
      "source": [
        "Confusion Matrix(**example**)\n",
        "\n",
        "In this section, we are calculating and visualizing the **confusion matrix** for the model's predictions on the test set.\n",
        "\n",
        "This step allows us to visually assess the performance of the classification model by identifying which classes are being correctly or incorrectly predicted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Jn669u9vVhgI",
        "outputId": "7a93e2d9-2157-4f66-901d-8b8cc9d502f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwwAAAJwCAYAAAAk6OZ1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAk+9JREFUeJzs3XlcFPXjx/H3grAoyuGSipoXGCAK4gmaYnkgHom3ZoqmZYalkmR4hGjfwCvTvPO+0swjU9PMKy3EAwWvzIPUFFBWBOVYdHd/fxT7Y4VFFtkZlnk/v495PL7MsvOancWND5+dHZlWq9WCiIiIiIioEBZi7wAREREREZVdHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQEREREZFBHDAQERXi2rVr6NKlC+zt7SGTybBr165S3f7ff/8NmUyGtWvXlup2zVmHDh3QoUMHsXeDiIiewwEDEZVZN27cwOjRo9GgQQPY2NjAzs4Obdu2xYIFC5CdnW3SdnBwMC5cuID//e9/2LBhA1q0aGHSnpCGDx8OmUwGOzu7Qo/jtWvXIJPJIJPJMHfuXKO3f+/ePUyfPh3nz58vhb0lIiKxVRB7B4iICrN37170798fcrkcw4YNQ+PGjZGbm4sTJ04gLCwMly5dwooVK0zSzs7ORkxMDKZMmYKxY8eapFG3bl1kZ2fDysrKJNt/kQoVKiArKws//fQTBgwYoHfbpk2bYGNjg5ycnBJt+969e4iMjES9evXQtGnTYt/vl19+KVGPiIhMiwMGIipzEhMTMWjQINStWxeHDx+Gs7Oz7raQkBBcv34de/fuNVn/wYMHAAAHBweTNWQyGWxsbEy2/ReRy+Vo27YtvvvuuwIDhs2bN6N79+7Yvn27IPuSlZWFSpUqwdraWpAeEREZh29JIqIyZ/bs2Xjy5AlWrVqlN1jI4+rqinHjxum+fvbsGWbOnAkXFxfI5XLUq1cPkydPhkql0rtfvXr10KNHD5w4cQKtWrWCjY0NGjRogPXr1+u+Z/r06ahbty4AICwsDDKZDPXq1QPw71t58v5/ftOnT4dMJtNbd/DgQbz++utwcHBA5cqV4ebmhsmTJ+tuN3QOw+HDh9GuXTvY2trCwcEBvXr1wpUrVwrtXb9+HcOHD4eDgwPs7e0xYsQIZGVlGT6wz3n77bfx888/49GjR7p1p0+fxrVr1/D2228X+P6HDx9i4sSJaNKkCSpXrgw7OzsEBgYiPj5e9z1Hjx5Fy5YtAQAjRozQvbUp73F26NABjRs3xtmzZ9G+fXtUqlRJd1yeP4chODgYNjY2BR5/QEAAHB0dce/evWI/ViIiKjkOGIiozPnpp5/QoEEDtGnTpljfP2rUKHz++edo1qwZ5s+fD39/f0RFRWHQoEEFvvf69evo168fOnfujHnz5sHR0RHDhw/HpUuXAAB9+vTB/PnzAQCDBw/Ghg0b8PXXXxu1/5cuXUKPHj2gUqkwY8YMzJs3D2+99RZ+//33Iu/366+/IiAgAPfv38f06dMRGhqKP/74A23btsXff/9d4PsHDBiAx48fIyoqCgMGDMDatWsRGRlZ7P3s06cPZDIZduzYoVu3efNmuLu7o1mzZgW+/+bNm9i1axd69OiBr776CmFhYbhw4QL8/f11v7x7eHhgxowZAID3338fGzZswIYNG9C+fXvddpRKJQIDA9G0aVN8/fXXeOONNwrdvwULFuCVV15BcHAw1Go1AGD58uX45Zdf8M0336BmzZrFfqxERPQStEREZUh6eroWgLZXr17F+v7z589rAWhHjRqlt37ixIlaANrDhw/r1tWtW1cLQPvbb7/p1t2/f18rl8u1n3zyiW5dYmKiFoB2zpw5etsMDg7W1q1bt8A+REREaPO/nM6fP18LQPvgwQOD+53XWLNmjW5d06ZNtdWqVdMqlUrduvj4eK2FhYV22LBhBXrvvvuu3jZ79+6tVSgUBpv5H4etra1Wq9Vq+/Xrp+3YsaNWq9Vq1Wq1tkaNGtrIyMhCj0FOTo5WrVYXeBxyuVw7Y8YM3brTp08XeGx5/P39tQC0y5YtK/Q2f39/vXUHDhzQAtB+8cUX2ps3b2orV66sDQoKeuFjJCKi0sMZBiIqUzIyMgAAVapUKdb379u3DwAQGhqqt/6TTz4BgALnOjRq1Ajt2rXTff3KK6/Azc0NN2/eLPE+Py/v3Icff/wRGo2mWPdJSkrC+fPnMXz4cFStWlW33svLC507d9Y9zvw++OADva/btWsHpVKpO4bF8fbbb+Po0aNITk7G4cOHkZycXOjbkYB/z3uwsPj3PxtqtRpKpVL3dqu4uLhiN+VyOUaMGFGs7+3SpQtGjx6NGTNmoE+fPrCxscHy5cuL3SIiopfHAQMRlSl2dnYAgMePHxfr+2/dugULCwu4urrqra9RowYcHBxw69YtvfV16tQpsA1HR0ekpaWVcI8LGjhwINq2bYtRo0ahevXqGDRoEL7//vsiBw95++nm5lbgNg8PD6SmpiIzM1Nv/fOPxdHREQCMeizdunVDlSpVsHXrVmzatAktW7YscCzzaDQazJ8/Hw0bNoRcLoeTkxNeeeUVJCQkID09vdjNWrVqGXWC89y5c1G1alWcP38eCxcuRLVq1Yp9XyIienkcMBBRmWJnZ4eaNWvi4sWLRt3v+ZOODbG0tCx0vVarLXEj7/31eSpWrIjffvsNv/76K4YOHYqEhAQMHDgQnTt3LvC9L+NlHkseuVyOPn36YN26ddi5c6fB2QUA+PLLLxEaGor27dtj48aNOHDgAA4ePAhPT89iz6QA/x4fY5w7dw73798HAFy4cMGo+xIR0cvjgIGIypwePXrgxo0biImJeeH31q1bFxqNBteuXdNbn5KSgkePHuk+8ag0ODo66n2iUJ7nZzEAwMLCAh07dsRXX32Fy5cv43//+x8OHz6MI0eOFLrtvP28evVqgdv+/PNPODk5wdbW9uUegAFvv/02zp07h8ePHxd6onieH374AW+88QZWrVqFQYMGoUuXLujUqVOBY1LcwVtxZGZmYsSIEWjUqBHef/99zJ49G6dPny617RMR0YtxwEBEZc6nn34KW1tbjBo1CikpKQVuv3HjBhYsWADg37fUACjwSUZfffUVAKB79+6ltl8uLi5IT09HQkKCbl1SUhJ27typ930PHz4scN+8C5g9/1GveZydndG0aVOsW7dO7xfwixcv4pdfftE9TlN44403MHPmTCxatAg1atQw+H2WlpYFZi+2bduGu3fv6q3LG9gUNrgy1qRJk3D79m2sW7cOX331FerVq4fg4GCDx5GIiEofL9xGRGWOi4sLNm/ejIEDB8LDw0PvSs9//PEHtm3bhuHDhwMAvL29ERwcjBUrVuDRo0fw9/fHqVOnsG7dOgQFBRn8yM6SGDRoECZNmoTevXvj448/RlZWFpYuXYrXXntN76TfGTNm4LfffkP37t1Rt25d3L9/H0uWLEHt2rXx+uuvG9z+nDlzEBgYCD8/P4wcORLZ2dn45ptvYG9vj+nTp5fa43iehYUFpk6d+sLv69GjB2bMmIERI0agTZs2uHDhAjZt2oQGDRrofZ+LiwscHBywbNkyVKlSBba2tmjdujXq169v1H4dPnwYS5YsQUREhO5jXtesWYMOHTpg2rRpmD17tlHbIyKikuEMAxGVSW+99RYSEhLQr18//PjjjwgJCcFnn32Gv//+G/PmzcPChQt137ty5UpERkbi9OnTGD9+PA4fPozw8HBs2bKlVPdJoVBg586dqFSpEj799FOsW7cOUVFR6NmzZ4F9r1OnDlavXo2QkBAsXrwY7du3x+HDh2Fvb29w+506dcL+/fuhUCjw+eefY+7cufD19cXvv/9u9C/bpjB58mR88sknOHDgAMaNG4e4uDjs3bsXr776qt73WVlZYd26dbC0tMQHH3yAwYMH49ixY0a1Hj9+jHfffRc+Pj6YMmWKbn27du0wbtw4zJs3DydPniyVx0VEREWTaY05O46IiIiIiCSFMwxERERERGQQBwxERERERGQQBwxERERERGQQBwxERERERGZg6dKl8PLygp2dHezs7ODn54eff/65yPts27YN7u7usLGxQZMmTbBv3z6juxwwEBERERGZgdq1ayM6Ohpnz57FmTNn8Oabb6JXr164dOlSod//xx9/YPDgwRg5ciTOnTuHoKAgBAUF4eLFi0Z1+SlJRERERERmqmrVqpgzZw5GjhxZ4LaBAwciMzMTe/bs0a3z9fVF06ZNsWzZsmI3OMNARERERCQSlUqFjIwMvaU4V7NXq9XYsmULMjMz4efnV+j3xMTEoFOnTnrrAgICEBMTY9Q+lssrPftGG3eBoNJ0dKK/aG0x7bmUJFq7h6ezaG0iIiIqGZsy/FtoRZ+xgrUm9XJCZGSk3rqIiAhMnz690O+/cOEC/Pz8kJOTg8qVK2Pnzp1o1KhRod+bnJyM6tWr662rXr06kpOTjdrHMvxUERERERGVb+Hh4QgNDdVbJ5fLDX6/m5sbzp8/j/T0dPzwww8IDg7GsWPHDA4aSgMHDERERERE+cmEe9e+XC4vcoDwPGtra7i6ugIAmjdvjtOnT2PBggVYvnx5ge+tUaMGUlJS9NalpKSgRo0aRu0jz2EgIiIiIjJTGo3G4DkPfn5+OHTokN66gwcPGjznwRDOMBARERER5SeTib0HhQoPD0dgYCDq1KmDx48fY/PmzTh69CgOHDgAABg2bBhq1aqFqKgoAMC4cePg7++PefPmoXv37tiyZQvOnDmDFStWGNXlgIGIiIiIyAzcv38fw4YNQ1JSEuzt7eHl5YUDBw6gc+fOAIDbt2/DwuL/30DUpk0bbN68GVOnTsXkyZPRsGFD7Nq1C40bNzaqywEDEREREVF+Ap7DYIxVq1YVefvRo0cLrOvfvz/69+//Ut2yeTSIiIiIiKhM4IABwDDfV7E62AeHJrTFvo/8MKuPJ+pUrSjoPmzZvAmBnd9ES58mGDKoPy4kJJT7duLleKyPDkf06L6YMqADLp86Lkg3jxSPOdtss80222ybe1sQMplwixnggAGATx0HbI+7h1EbzuHjrQmoYCHDgoFesLES5vDs/3kf5s6OwugPQ7Bl2064ubljzOiRUCqV5bqdq8qBcz0X9Bw53uSt50n1mLPNNttss822ObdJHBwwAJjw/QXsvZCCxNQsXL+fiZl7r8LZ3gbuNaoI0t+wbg369BuAoN594eLqiqkRkbCxscGuHdvLddvNpzU6DxoFz1btTN56nlSPOdtss80222ybc1swMgvhFjMg6l6mpqZi9uzZ6N27N/z8/ODn54fevXtjzpw5ePDggWj7VVluCQDIyH5q8tbT3FxcuXwJvn5tdOssLCzg69sGCfHnym1bTFI95myzzTbbbLNtzm0Sj2gDhtOnT+O1117DwoULYW9vj/bt26N9+/awt7fHwoUL4e7ujjNnzrxwOyqVChkZGXqL5lluifdLBmB8J1fE30nHzdSsEm+nuNIepUGtVkOhUOitVygUSE1NLbdtMUn1mLPNNttss822ObcFxXMY9Ij2saofffQR+vfvj2XLlkH23MHSarX44IMP8NFHHyEmJqbI7URFRSEyMlJvXa2OwajdaUSJ9iusS0O4vGKL9zdylExEREREJNoMQ3x8PCZMmFBgsAAAMpkMEyZMwPnz51+4nfDwcKSnp+stNTsMKdE+fdLZFW1dq+LDzfF48LjksxTGcHRwhKWlZYEThZRKJZycnMptW0xSPeZss80222yzbc5tQfEcBj2i7WWNGjVw6tQpg7efOnUK1atXf+F25HI57Ozs9BaLCtZG788nnV3h/5oTxn6XgKT0HKPvX1JW1tbwaOSJ2JP/P5Oi0WgQGxsDL2+fctsWk1SPOdtss80222ybc5vEI9pbkiZOnIj3338fZ8+eRceOHXWDg5SUFBw6dAjffvst5s6dK8i+hHVxRZdG1fHp9ovIzH2GqrZWAIBMlRqqZxqT94cGj8C0yZPg6dkYjZt4YeOGdcjOzkZQ7z7luq3KyYIy+a7u67T7ybj39zVUqmwHB6cXDxZfhlSPOdtss80222ybc5vEIdqAISQkBE5OTpg/fz6WLFkCtVoNALC0tETz5s2xdu1aDBgwQJB96dusFgBg6ZCmeutn7v0Tey+kmLzfNbAb0h4+xJJFC5Ga+gBu7h5YsnwlFAJM7YnZvnvjKlZFTtB9vW/9YgCAj38A+oWEm7Qt1WPONttss8022+bcFoyZnIwsFJlWq9WKvRNPnz7VnVnv5OQEKyurl9qeb/Sx0titEjk60V+0tpj2XEoSrd3D01m0NhEREZWMjWh/tn6xin6fCdbKjokWrFVSZeKpsrKygrMzf+kjIiIiojLATE5GFgqPBhERERERGVQmZhiIiIiIiMoMnsOghzMMRERERERkEGcYiIiIiIjy4zkMeng0iIiIiIjIIM4wEBERERHlx3MY9HCGgYiIiIiIDCqXMwxSvXiamMS8eBovGkdSwJ9zkgL+nFOZwXMY9PBoEBERERGRQeVyhoGIiIiIqMQ4w6CHR4OIiIiIiAziDAMRERERUX4W/JSk/DjDQEREREREBnGGgYiIiIgoP57DoIdHg4iIiIiIDOKAIZ8tmzchsPObaOnTBEMG9ceFhAS2y2k78XI81keHI3p0X0wZ0AGXTx0XpJtHisecbf6cS+GYs82fcykcc7HbJDwOGP6z/+d9mDs7CqM/DMGWbTvh5uaOMaNHQqlUsl0O27mqHDjXc0HPkeNN3nqeVI852/w5l8IxZ5s/51I45mK2BSOTCbeYAQ4Y/rNh3Rr06TcAQb37wsXVFVMjImFjY4NdO7azXQ7bbj6t0XnQKHi2amfy1vOkeszZ5s+5FI452/w5l8IxF7NN4uCAAcDT3FxcuXwJvn5tdOssLCzg69sGCfHn2C5nbTFJ9ZizzZ9zKRxztvlzLoVjLpnnW2Yh3GIGyvRe3rlzB++++26R36NSqZCRkaG3qFQqozppj9KgVquhUCj01isUCqSmphq932yX7baYpHrM2ebPOVD+jznb/DkHyv8xl+rzLXVlesDw8OFDrFu3rsjviYqKgr29vd4yZ1aUQHtIREREROUOz2HQI+p1GHbv3l3k7Tdv3nzhNsLDwxEaGqq3TmspN2o/HB0cYWlpWeBkHaVSCScnJ6O2ZSy2hW+LSarHnG3+nAPl/5izzZ9zoPwfc6k+31In6gxDUFAQevfujaCgoEKX5wcChZHL5bCzs9Nb5HLjBgxW1tbwaOSJ2JMxunUajQaxsTHw8vYx+nGxXbbbYpLqMWebP+dSOOZs8+dcCsdcMs83z2HQI+oMg7OzM5YsWYJevXoVevv58+fRvHlzQfZlaPAITJs8CZ6ejdG4iRc2bliH7OxsBPXuw3Y5bKtysqBMvqv7Ou1+Mu79fQ2VKtvBwam6SdtSPeZs8+dcCsecbf6cS+GYi9kmcYg6YGjevDnOnj1rcMAgk8mg1WoF2Zeugd2Q9vAhlixaiNTUB3Bz98CS5SuhEGB6jW3h23dvXMWqyAm6r/etXwwA8PEPQL+QcJO2pXrM2ebPuRSOOdv8OZfCMRezLRgzObdAKDKtUL+RF+L48ePIzMxE165dC709MzMTZ86cgb+/v1HbzXlWGntH5mLPpSTR2j08nUVrk7Tw55ykgD/n0mIj6p+ti1YxYK5grewDEwVrlZSoT1W7dkVfZMXW1tbowQIRERER0Usxk3MLhMKjQUREREREBpXhySAiIiIiIhHwHAY9nGEgIiIiIiKDOMNARERERJQfz2HQw6NBREREREQGcYaBiIiIiCg/nsOghzMMRERERERkkKgXbjMVqV64jRe8ISIiMj9i/vdbTP28y+7vDhV7LBKslb1nrGCtkuIMAxERERERGcQBAxERERERGcSTnomIiIiI8uPHqurh0SAiIiIiIoM4w0BERERElB8/VlUPZxiIiIiIiMggzjAQEREREeXHcxj08Gjks2XzJgR2fhMtfZpgyKD+uJCQUO7biZfjsT46HNGj+2LKgA64fOq4IN08UjzmbLPNNttss/2yxPzvt9i/O5DwOGD4z/6f92Hu7CiM/jAEW7bthJubO8aMHgmlUlmu27mqHDjXc0HPkeNN3nqeVI8522yzzTbbbL8sMf/7LWZbMDKZcIsZ4IDhPxvWrUGffgMQ1LsvXFxdMTUiEjY2Nti1Y3u5brv5tEbnQaPg2aqdyVvPk+oxZ5ttttlmm+2XJeZ/v8Vskzg4YADwNDcXVy5fgq9fG906CwsL+Pq2QUL8uXLbFpNUjznbbLPNNttskxmQWQi3mAHR9zI7OxsnTpzA5cuXC9yWk5OD9evXF3l/lUqFjIwMvUWlUhm1D2mP0qBWq6FQKPTWKxQKpKamGrUtY4nZFpNUjznbbLPNNttsE5kbUQcMf/31Fzw8PNC+fXs0adIE/v7+SEpK0t2enp6OESNGFLmNqKgo2Nvb6y1zZkWZeteJiIiIqLziOQx6RB0wTJo0CY0bN8b9+/dx9epVVKlSBW3btsXt27eLvY3w8HCkp6frLWGTwo3aD0cHR1haWhY4SUmpVMLJycmobRlLzLaYpHrM2WabbbbZZpvI3Ig6YPjjjz8QFRUFJycnuLq64qeffkJAQADatWuHmzdvFmsbcrkcdnZ2eotcLjdqP6ysreHRyBOxJ2N06zQaDWJjY+Dl7WPUtowlZltMUj3mbLPNNttss01ln0wmE2wxB6JeuC07OxsVKvz/LshkMixduhRjx46Fv78/Nm/eLNi+DA0egWmTJ8HTszEaN/HCxg3rkJ2djaDefcp1W5WTBWXyXd3XafeTce/va6hU2Q4OTtVN2pbqMWebbbbZZpvtlyXmf7/FbJM4RB0wuLu748yZM/Dw8NBbv2jRIgDAW2+9Jdi+dA3shrSHD7Fk0UKkpj6Am7sHlixfCYUA04pitu/euIpVkRN0X+9bvxgA4OMfgH4hxr21y1hSPeZss80222yz/bLE/O+3mG2hmMtf/oUi02q1WrHiUVFROH78OPbt21fo7R9++CGWLVsGjUZj1HZznpXG3pmfPZeSXvxNJtLD01m0NhERkTkT87/fYurnXXZ/d7Dtt0awVuYPRX/AT1kg6jkM4eHhBgcLALBkyRKjBwtERERERC9FJuBiBkS/DgMREREREZVdHDAQEREREZFBop70TERERERU1vCkZ32cYSAiIiIiIoM4w0BERERElA9nGPRxhoGIiIiIiAziDAMRERERUT6cYdDHAUM5wounERGVng5zj4nWPjrRX7Q2CU/M/35L9aJxZBwOGIiIiIiI8uEMgz6ew0BERERERAZxhoGIiIiIKD9OMOjhDAMRERERERnEAQMRERERUT4ymUywxRhRUVFo2bIlqlSpgmrVqiEoKAhXr14t8j5r164t0LSxsTGqywEDEREREZEZOHbsGEJCQnDy5EkcPHgQT58+RZcuXZCZmVnk/ezs7JCUlKRbbt26ZVSX5zAQEREREeVTVj8laf/+/Xpfr127FtWqVcPZs2fRvn17g/eTyWSoUaNGibucYchny+ZNCOz8Jlr6NMGQQf1xISGBbbbZZptttotlmO+rWB3sg0MT2mLfR36Y1ccTdapWNHk3P6kdc7bFaSdejsf66HBEj+6LKQM64PKp44J0yyuVSoWMjAy9RaVSFeu+6enpAICqVasW+X1PnjxB3bp18eqrr6JXr164dOmSUfvIAcN/9v+8D3NnR2H0hyHYsm0n3NzcMWb0SCiVSrbZZpttttl+IZ86Dtgedw+jNpzDx1sTUMFChgUDvWBjJcx/aqV4zNkWp52ryoFzPRf0HDne5C2xCHkOQ1RUFOzt7fWWqKioF+6jRqPB+PHj0bZtWzRu3Njg97m5uWH16tX48ccfsXHjRmg0GrRp0wb//PNP8Y+HVqvVFvu7zUTOM+PvM2RQf3g2boLJUz8H8O+T0KWjPwa/PRQj33u/lPeQbbbZZpvtst5+2Ss9O1S0wv5xbfDBpvM4fyfdqPuW5ErP5eGYsy18+2Wv9DxlQAcMmTgTjVq1M/q+/bzFu8L1i1QdulmwVtLKvgVmFORyOeRyeZH3GzNmDH7++WecOHECtWvXLnbv6dOn8PDwwODBgzFz5sxi3YczDACe5ubiyuVL8PVro1tnYWEBX982SIg/xzbbbLPNNttGqyy3BABkZD81eUuqx5xt8X/OyyshZxjkcjns7Oz0lhcNFsaOHYs9e/bgyJEjRg0WAMDKygo+Pj64fv16se8j+oDhypUrWLNmDf78808AwJ9//okxY8bg3XffxeHDh194/5d531eetEdpUKvVUCgUeusVCgVSU1ON2pax2GabbbbZLh/t/GQAxndyRfyddNxMzTJ5T6rHnG1xf85JeFqtFmPHjsXOnTtx+PBh1K9f3+htqNVqXLhwAc7OxZ/hEXXAsH//fjRt2hQTJ06Ej48P9u/fj/bt2+P69eu4desWunTp8sJBQ2Hv+5oz68Xv+yIiIjKVsC4N4fKKLabuviz2rhBRScgEXIwQEhKCjRs3YvPmzahSpQqSk5ORnJyM7Oxs3fcMGzYM4eHhuq9nzJiBX375BTdv3kRcXBzeeecd3Lp1C6NGjSp2V9QBw4wZMxAWFgalUok1a9bg7bffxnvvvYeDBw/i0KFDCAsLQ3R0dJHbCA8PR3p6ut4SNim8yPs8z9HBEZaWlgVOFFIqlXBycjL6cbHNNttssy29dp5POruirWtVfLg5Hg8e5wrSlOoxZ1u8n3MSx9KlS5Geno4OHTrA2dlZt2zdulX3Pbdv30ZS0v+fm5KWlob33nsPHh4e6NatGzIyMvDHH3+gUaNGxe6KOmC4dOkShg8fDgAYMGAAHj9+jH79+uluHzJkCBJe8BFhJXnf1/OsrK3h0cgTsSdjdOs0Gg1iY2Pg5e1j1LaMxTbbbLPNdvloA/8OFvxfc8LY7xKQlJ5j8l4eqR5ztsX5OSfxaLXaQpe836cB4OjRo1i7dq3u6/nz5+PWrVtQqVRITk7G3r174eNj3M+J6Bduy7swhoWFBWxsbGBvb6+7rUqVKrrPlzW1ocEjMG3yJHh6NkbjJl7YuGEdsrOzEdS7D9tss80222y/UFgXV3RpVB2fbr+IzNxnqGprBQDIVKmheqYxaRuQ5jFnW5y2KicLyuS7uq/T7ifj3t/XUKmyHRycqpu8L4SyeuE2sYg6YKhXrx6uXbsGFxcXAEBMTAzq1Kmju/327dtGnZDxMroGdkPaw4dYsmghUlMfwM3dA0uWr4RCgKk9ttlmm222zb/dt1ktAMDSIU311s/c+yf2XkgxaRuQ5jFnW5z23RtXsSpygu7rfesXAwB8/APQL8S4t4WTeRD1OgzLli3Dq6++iu7duxd6++TJk3H//n2sXLnSqO2W5DoMRERE+b3sdRheRkmuw0BUEi97HYaXUZavw/DKiK0v/qZS8mDNQMFaJSXqDMMHH3xQ5O1ffvmlQHtCRERERESFEf0cBiIiIiKisoTnMOgT/cJtRERERERUdnGGgYiIiIgoP04w6OEMAxERERERGcQZBiIiIiKifHgOgz7OMBARERERkUGcYSAiIiIiyoczDPo4YCCzx4srEZEp8N+38MS8iFgPz7J7ETFTkurjJuNwwEBERERElA9nGPTxHAYiIiIiIjKIMwxERERERPlwhkEfZxiIiIiIiMggzjAQEREREeXHCQY9nGEgIiIiIiKDOGAgIiIiIiKDOGDIZ8vmTQjs/CZa+jTBkEH9cSEhge1y2B7m+ypWB/vg0IS22PeRH2b18USdqhVN3s1PasecbbbZZttUEi/HY310OKJH98WUAR1w+dRxQbp5pHjMxW4LQSaTCbaYAw4Y/rP/532YOzsKoz8MwZZtO+Hm5o4xo0dCqVSyXc7aPnUcsD3uHkZtOIePtyaggoUMCwZ6wcZKmH8OUjzmbLPNNtumkqvKgXM9F/QcOd7kredJ9ZiL2SZxlLkBg1arFaW7Yd0a9Ok3AEG9+8LF1RVTIyJhY2ODXTu2s13O2hO+v4C9F1KQmJqF6/czMXPvVTjb28C9RhWTdvNI8ZizzTbbbJuKm09rdB40Cp6t2pm89TypHnMx20LhDIO+MjdgkMvluHLliqDNp7m5uHL5Enz92ujWWVhYwNe3DRLiz7FdztrPqyy3BABkZD81eUuqx5xtttlmu7yR6jGX6vMtdaJ9rGpoaGih69VqNaKjo6FQKAAAX331VZHbUalUUKlUeuu0lnLI5fJi70vaozSo1WpdM49CoUBi4s1ib6ck2Ba+nZ8MwPhOroi/k46bqVkm70n1mLPNNttslzdSPeZSeb7N5S//QhFtwPD111/D29sbDg4Oeuu1Wi2uXLkCW1vbYj1ZUVFRiIyM1Fs3ZVoEpn4+vRT3lsqrsC4N4fKKLd7fyL+KEBERERVGtAHDl19+iRUrVmDevHl48803deutrKywdu1aNGrUqFjbCQ8PLzBbobUs/uwCADg6OMLS0rLAyTpKpRJOTk5GbctYbAvfzvNJZ1e0da2KDzbF48HjXEGaUj3mbLPNNtvljVSPuWSeb04w6BHtHIbPPvsMW7duxZgxYzBx4kQ8fVqy94/L5XLY2dnpLca8HQkArKyt4dHIE7EnY3TrNBoNYmNj4OXtU6L9YrvstoF/Bwv+rzlh7HcJSErPMXkvj1SPOdtss812eSPVYy7V51vqRJthAICWLVvi7NmzCAkJQYsWLbBp0ybR3jM2NHgEpk2eBE/PxmjcxAsbN6xDdnY2gnr3YbuctcO6uKJLo+r4dPtFZOY+Q1VbKwBApkoN1TONSduANI8522yzzbapqHKyoEy+q/s67X4y7v19DZUq28HBqbpJ21I95mK2hcJzGPSJOmAAgMqVK2PdunXYsmULOnXqBLVaLcp+dA3shrSHD7Fk0UKkpj6Am7sHlixfCYUA02tsC9vu26wWAGDpkKZ662fu/RN7L6SYtA1I85izzTbbbJvK3RtXsSpygu7rfesXAwB8/APQLyTcpG2pHnMx2yQOmVasCx8U4p9//sHZs2fRqVMn2Nralng7Oc9KcaeozOsw95ho7aMT/UVrExGVN3suJYnW7uHpLFpbqmxE/7O1YXU//kmw1q2FPQVrlVSZeqpq166N2rVri70bRERERET0nzI1YCAiIiIiEhvPYdBX5q70TEREREREZQdnGIiIiIiI8uEMgz7OMBARERERkUGcYSAiIiIiyo8TDHo4w0BERERERAZxhoHM3sTA10Rr8zPDiYiIyh+ew6CPMwxERERERGQQBwxERERERGQQ35JERERERJQP35KkjzMMRERERERkEGcYiIiIiIjy4QSDPs4wEBERERGRQZxhICIiIiLKh+cw6OMMAxERERERGcQBQz5bNm9CYOc30dKnCYYM6o8LCQlsl9N24uV4rI8OR/TovpgyoAMunzouSFfsNiDN55ttttkuv22+pkqvLQSZTLjFHHDA8J/9P+/D3NlRGP1hCLZs2wk3N3eMGT0SSqWS7XLYzlXlwLmeC3qOHG/yVllqS/X5Zptttstvm6+p0mqTODhg+M+GdWvQp98ABPXuCxdXV0yNiISNjQ127djOdjlsu/m0RudBo+DZqp3JW2WpLdXnm2222S6/bb6mSqstFJlMJthiDjhgAPA0NxdXLl+Cr18b3ToLCwv4+rZBQvw5tstZW6qk+nyzzTbb5bctJqkec6k+31JXpgYMmZmZWLNmDaZMmYJFixYVa2pLpVIhIyNDb1GpVEZ10x6lQa1WQ6FQ6K1XKBRITU01alvGYlv4tlRJ9flmm222y29bTFI95lJ5vnkOgz5RBwyNGjXCw4cPAQB37txB48aNMWHCBBw8eBARERFo1KgREhMTi9xGVFQU7O3t9ZY5s6KE2H0iIiIionJP1Osw/Pnnn3j27BkAIDw8HDVr1sT58+dhb2+PJ0+eoHfv3pgyZQo2b95scBvh4eEIDQ3VW6e1lBu1H44OjrC0tCwwo6FUKuHk5GTUtozFtvBtqZLq880222yX37aYpHrMpfJ8W1iYyZ/+BVJm3pIUExOD6dOnw97eHgBQuXJlREZG4sSJE0XeTy6Xw87OTm+Ry40bMFhZW8OjkSdiT8bo1mk0GsTGxsDL28f4B8N2mW5LlVSfb7bZZrv8tsUk1WMu1edb6kS/0nPe2eE5OTlwdnbWu61WrVp48OCBIPsxNHgEpk2eBE/PxmjcxAsbN6xDdnY2gnr3YbsctlU5WVAm39V9nXY/Gff+voZKle3g4FS93Lal+nyzzTbb5bfN11RptYViLucWCEX0AUPHjh1RoUIFZGRk4OrVq2jcuLHutlu3bhU4qcZUugZ2Q9rDh1iyaCFSUx/Azd0DS5avhEKA6TW2hW/fvXEVqyIn6L7et34xAMDHPwD9QsLLbVuqzzfbbLNdftt8TZVWm8Qh02q1WrHikZGRel/7+voiICBA93VYWBj++ecffPfdd0ZtN+dZqewemYk9l5LE3gVR9PB0fvE3ERGZETFfz/maKjwb0f9sbVjjqQcFa138orNgrZIS9amKiIgo8vY5c+YItCdERERERFSYMnPSMxERERERlT1leDKIiIiIiEh4POlZH2cYiIiIiIjIIM4wEBERERHlI+MUgx7OMBARERERkUGcYSAiIiIiyoczDPo4YCCzJ9XPzubnlZMU8OeciEh8HDAQEREREeXDCQZ9PIeBiIiIiIgM4gwDEREREVE+PIdBH2cYiIiIiIjIIM4wEBERERHlwwkGfZxhICIiIiIigzjDQERERESUD89h0McZBiIiIiIiMogDhny2bN6EwM5voqVPEwwZ1B8XEhLYZrtctRMvx2N9dDiiR/fFlAEdcPnUcUG6eaR4zNnmz7kUjrmYbT7f0msLQSYTbjEHHDD8Z//P+zB3dhRGfxiCLdt2ws3NHWNGj4RSqWSb7XLTzlXlwLmeC3qOHG/y1vOkeszZ5s+5FI45n29pHXMx2yQODhj+s2HdGvTpNwBBvfvCxdUVUyMiYWNjg107trPNdrlpu/m0RudBo+DZqp3JW8+T6jFnmz/nUjjmfL6ldczFbAtFJpMJtpgDDhgAPM3NxZXLl+Dr10a3zsLCAr6+bZAQf45ttstFW0xSPeZs8+dcCsecz7e0jrlUn2+pE3XAEBcXh8TERN3XGzZsQNu2bfHqq6/i9ddfx5YtW164DZVKhYyMDL1FpVIZtR9pj9KgVquhUCj01isUCqSmphq1LWOxzbZQbTFJ9ZizzZ9zoPwfcz7f0jrmUnm+eQ6DPlEHDCNGjMCNGzcAACtXrsTo0aPRokULTJkyBS1btsR7772H1atXF7mNqKgo2Nvb6y1zZkUJsftEREREROWeqAOGa9euoWHDhgCAJUuWYMGCBViwYAE++OADzJ8/H8uXL8e8efOK3EZ4eDjS09P1lrBJ4Ubth6ODIywtLQucrKNUKuHk5GTcgzIS22wL1RaTVI852/w5B8r/MefzLa1jLtXnu6yIiopCy5YtUaVKFVSrVg1BQUG4evXqC++3bds2uLu7w8bGBk2aNMG+ffuM6oo6YKhUqZJu+uru3bto1aqV3u2tW7fWe8tSYeRyOezs7PQWuVxu1H5YWVvDo5EnYk/G6NZpNBrExsbAy9vHqG0Zi222hWqLSarHnG3+nEvhmPP5ltYxl8rzXVZPej527BhCQkJw8uRJHDx4EE+fPkWXLl2QmZlp8D5//PEHBg8ejJEjR+LcuXMICgpCUFAQLl68WOyuqFd6DgwMxNKlS7Fy5Ur4+/vjhx9+gLe3t+7277//Hq6uroLsy9DgEZg2eRI8PRujcRMvbNywDtnZ2Qjq3YdttstNW5WTBWXyXd3XafeTce/va6hU2Q4OTtVN2pbqMWebP+dSOOZ8vqV1zMVsl0cqlarA+bdyubzQP4Dv379f7+u1a9eiWrVqOHv2LNq3b1/o9hcsWICuXbsiLCwMADBz5kwcPHgQixYtwrJly4q1j6IOGGbNmoW2bdvC398fLVq0wLx583D06FF4eHjg6tWrOHnyJHbu3CnIvnQN7Ia0hw+xZNFCpKY+gJu7B5YsXwmFANNrbLMtVPvujatYFTlB9/W+9YsBAD7+AegXYtxb+Ywl1WPONn/OpXDM+XxL65iL2RaKkCcjR0VFITIyUm9dREQEpk+f/sL7pqenAwCqVq1q8HtiYmIQGhqqty4gIAC7du0q9j7KtFqtttjfbQKPHj1CdHQ0fvrpJ9y8eRMajQbOzs5o27YtJkyYgBYtWhi9zZxnJthRojJmz6Uk0do9PJ1Fa5O08OdcWvh8S4uNqH+2Lppv9DHBWscm+BZ7hiE/jUaDt956C48ePcKJEycMfp+1tTXWrVuHwYMH69YtWbIEkZGRSElJKdY+iv5UOTg4IDo6GtHR0WLvChERERGRoBdUK87goDAhISG4ePFikYOF0iL6gIGIiIiIiIpv7Nix2LNnD3777TfUrl27yO+tUaNGgZmElJQU1KhRo9g9XumZiIiIiCifsnrhNq1Wi7Fjx2Lnzp04fPgw6tev/8L7+Pn54dChQ3rrDh48CD8/v2J3OcNARERERGQGQkJCsHnzZvz444+oUqUKkpOTAQD29vaoWLEiAGDYsGGoVasWoqL+vZDxuHHj4O/vj3nz5qF79+7YsmULzpw5gxUrVhS7yxkGIiIiIqJ8yup1GJYuXYr09HR06NABzs7OumXr1q2677l9+zaSkv7/AwTatGmDzZs3Y8WKFfD29sYPP/yAXbt2oXHjxsXucoaBiIiIiMgMFOfDTY8ePVpgXf/+/dG/f/8SdzlgICIiIiLKR8jrMJgDDhiIzJSYnxneYa5wn0/9vImBr4nW5ue0C4/HXFr4fBOVTRwwEBERERHlI+R1GMwBT3omIiIiIiKDOMNARERERJQPZxj0cYaBiIiIiIgM4gwDEREREVE+nGDQxxkGIiIiIiIyiAMGIiIiIiIyiAOGfLZs3oTAzm+ipU8TDBnUHxcSEthmm+1SMMz3VawO9sGhCW2x7yM/zOrjiTpVK5q8myfxcjzWR4cjenRfTBnQAZdPHResDUjv+WabbbbZNncymUywxRxwwPCf/T/vw9zZURj9YQi2bNsJNzd3jBk9Ekqlkm222X5JPnUcsD3uHkZtOIePtyaggoUMCwZ6wcZKmJegXFUOnOu5oOfI8YL08pPi880222yzTeULBwz/2bBuDfr0G4Cg3n3h4uqKqRGRsLGxwa4d29lmm+2XNOH7C9h7IQWJqVm4fj8TM/dehbO9DdxrVDFpN4+bT2t0HjQKnq3aCdLLT4rPN9tss822uZPJhFvMAQcMAJ7m5uLK5Uvw9WujW2dhYQFf3zZIiD/HNttsl7LKcksAQEb2U0G7QpPq880222yzTeULBwwA0h6lQa1WQ6FQ6K1XKBRITU1lm222S5EMwPhOroi/k46bqVmCdcUg1eebbbbZZtvc8RwGfaIOGD766CMcP/5yJx+qVCpkZGToLSqVqpT2kIhKW1iXhnB5xRZTd18We1eIiIioGEQdMCxevBgdOnTAa6+9hlmzZiE5OdnobURFRcHe3l5vmTMryqhtODo4wtLSssDJOkqlEk5OTkbvE9tss124Tzq7oq1rVXy4OR4PHucK0hSTVJ9vttlmm21zx3MY9In+lqRffvkF3bp1w9y5c1GnTh306tULe/bsgUajKdb9w8PDkZ6erreETQo3ah+srK3h0cgTsSdjdOs0Gg1iY2Pg5e1j1LaMxTbbUmgD/w4W/F9zwtjvEpCUnmPyXlkg1eebbbbZZpvKlwpi70CTJk3QsWNHzJkzBzt37sTq1asRFBSE6tWrY/jw4RgxYgRcXV0N3l8ul0Mul+uty3lm/H4MDR6BaZMnwdOzMRo38cLGDeuQnZ2NoN59jN8Y22yzrSesiyu6NKqOT7dfRGbuM1S1tQIAZKrUUD0r3h8HXoYqJwvK5Lu6r9PuJ+Pe39dQqbIdHJyqm7QtxeebbbbZZtvcWZjLn/4FIvqAIY+VlRUGDBiAAQMG4Pbt21i9ejXWrl2L6OhoqNVqk/e7BnZD2sOHWLJoIVJTH8DN3QNLlq+EQoDpNbbZLu/tvs1qAQCWDmmqt37m3j+x90KKSdsAcPfGVayKnKD7et/6xQAAH/8A9AsxbkbSWFJ8vtlmm222qXyRabVarVhxCwsLJCcno1q1aoXertVq8euvv6Jz585GbbckMwxEVHwd5h4TrT0x8DXR2j08nUVrExGVNzZl5s/WBXVZfFKw1i8hvoK1SkrUcxjq1q0LS0tLg7fLZDKjBwtERERERFR6RB3bJSYmipknIiIiIirAXK6PIBTRPyWJiIiIiIjKrjL87jEiIiIiIuFZcIJBD2cYiIiIiIjIIM4wEBERERHlw3MY9HGGgYiIiIiIDOIMAxERERFRPpxg0McBQynbcylJtDYvKkVCOTrRX7S2mP/GiIhMgb87UFnHtyQREREREZFBnGEgIiIiIspHBr4nKT/OMBARERERkUGcYSAiIiIiyocXbtPHGQYiIiIiIjKIMwxERERERPnwwm36OMNAREREREQGccCQz5bNmxDY+U209GmCIYP640JCgiDdxMvxWB8djujRfTFlQAdcPnVckG4esR4322zz3xjbbLPNdslI+XVNCDKZcIs54IDhP/t/3oe5s6Mw+sMQbNm2E25u7hgzeiSUSqXJ27mqHDjXc0HPkeNN3nqemI+bbbb5b4xtttlmu2Sk+rpG4uCA4T8b1q1Bn34DENS7L1xcXTE1IhI2NjbYtWO7ydtuPq3RedAoeLZqZ/LW88R83GyzzX9jbLPNNtslI9XXNaFYyGSCLeaAAwYAT3NzceXyJfj6tdGts7CwgK9vGyTEnxNxz0xLzMfNNtv8N1Z+jznbbLPN1zUqXzhgAJD2KA1qtRoKhUJvvUKhQGpqqkh7ZXpiPm622RaqLSapHnO22Wabr2vmjucw6BN9wLBo0SIMGzYMW7ZsAQBs2LABjRo1gru7OyZPnoxnz54VeX+VSoWMjAy9RaVSCbHrRERERETlnqgDhi+++AKTJ09GVlYWJkyYgFmzZmHChAkYMmQIgoODsXLlSsycObPIbURFRcHe3l5vmTMryqj9cHRwhKWlZYGTdZRKJZycnIx+XOZCzMfNNttCtcUk1WPONtts83XN3MlkMsEWcyDqgGHt2rVYu3YtfvjhB+zfvx9TpkzBggULMGXKFISHh2P58uXYvHlzkdsIDw9Henq63hI2Kdyo/bCytoZHI0/EnozRrdNoNIiNjYGXt0+JHps5EPNxs802/42V32PONtts83WNyhdRr/R87949tGjRAgDg7e0NCwsLNG3aVHd7s2bNcO/evSK3IZfLIZfL9dblFP0upkINDR6BaZMnwdOzMRo38cLGDeuQnZ2NoN59jN+YkVQ5WVAm39V9nXY/Gff+voZKle3g4FTdpG0xHzfbbPPfmGmxzTbb5bct1dc1oZjJH/4FI+qAoUaNGrh8+TLq1KmDa9euQa1W4/Lly/D09AQAXLp0CdWqVRNkX7oGdkPaw4dYsmghUlMfwM3dA0uWr4RCgOm1uzeuYlXkBN3X+9YvBgD4+AegX4hxsyXGEvNxs802/42ZFttss11+21J9XSNxyLRarVas+LRp07B8+XL06tULhw4dwsCBA7F582aEh4dDJpPhf//7H/r164evvvrKqO2WZIahtOy5lCRau4ens2htIqHw3xgRlTdSfV2zEfXP1kUbuE64j4jdGlz238ol6lMVGRmJihUrIiYmBu+99x4+++wzeHt749NPP0VWVhZ69uz5wpOeiYiIiIjIdEQdMFhYWGDy5Ml66wYNGoRBgwaJtEdERERERJRfsQYMCQkJxd6gl5dXiXeGiIiIiEhsPOdZX7EGDE2bNoVMJoOh0x3ybpPJZFCr1aW6g0REREREJJ5iDRgSExNNvR9ERERERGWCuVxQTSjFGjDUrVvX1PtBRERERERlUImu9Lxhwwa0bdsWNWvWxK1btwAAX3/9NX788cdS3TkiIiIiIqFZyIRbzIHRA4alS5ciNDQU3bp1w6NHj3TnLDg4OODrr78u7f0jIiIiIiIRGX3htkaNGuHLL79EUFAQqlSpgvj4eDRo0AAXL15Ehw4dkJqaaqp9LTYxL9xGROVXh7nHRGsfnegvWluqpHoxLSKhlOULt72zMV6w1sZ3vAVrlZTRMwyJiYnw8Sl4RTq5XI7MzMxS2SkiIiIiIiobjB4w1K9fH+fPny+wfv/+/fDw8CiNfSIiIiIiEo1MJtxiDoyeDAoNDUVISAhycnKg1Wpx6tQpfPfdd4iKisLKlStNsY9ERERERCQSowcMo0aNQsWKFTF16lRkZWXh7bffRs2aNbFgwQIMGjTIFPtIRERERCQYXodBX4lONxkyZAiGDBmCrKwsPHnyBNWqVSvt/SIiIiIiojKgxOen379/H1evXgXw7yjslVdeKbWdIiIiIiISi7lcH0EoRp/0/PjxYwwdOhQ1a9aEv78//P39UbNmTbzzzjtIT083xT4SEREREZFIjB4wjBo1CrGxsdi7dy8ePXqER48eYc+ePThz5gxGjx5tin0UzJbNmxDY+U209GmCIYP640JCAttss812iQ3zfRWrg31waEJb7PvID7P6eKJO1Yom7+YntWMudjvxcjzWR4cjenRfTBnQAZdPHRekm0eKx5xt6bWFIJPJBFvMgdEDhj179mD16tUICAiAnZ0d7OzsEBAQgG+//RY//fSTKfZREPt/3oe5s6Mw+sMQbNm2E25u7hgzeiSUSiXbbLPNdon41HHA9rh7GLXhHD7emoAKFjIsGOgFGyujX3pLRIrHXOx2rioHzvVc0HPkeJO3nifVY862tNokDqP/q6VQKGBvb19gvb29PRwdHUtlp8SwYd0a9Ok3AEG9+8LF1RVTIyJhY2ODXTu2s80222yXyITvL2DvhRQkpmbh+v1MzNx7Fc72NnCvUcWk3TxSPOZit918WqPzoFHwbNXO5K3nSfWYsy2ttlBkAi7mwOgBw9SpUxEaGork5GTduuTkZISFhWHatGmlunNCeZqbiyuXL8HXr41unYWFBXx92yAh/hzbbLPNdqmoLLcEAGRkPzV5S6rHvCw930KS6jFnW1ptEk+xPiXJx8dH7z1W165dQ506dVCnTh0AwO3btyGXy/HgwQOjzmNISkrC0qVLceLECSQlJcHCwgINGjRAUFAQhg8fDktLSyMfTsmkPUqDWq2GQqHQW69QKJCYeJNtttlm+6XJAIzv5Ir4O+m4mZpl8p5Uj3lZeb6FJtVjzra02kKyMJNzC4RSrAFDUFBQqYfPnDmDTp06wdXVFRUrVsS1a9fw9ttvIzc3FxMnTsTq1auxf/9+VKlS9NS9SqWCSqXSW6e1lEMul5f6PhMRlVRYl4ZwecUW72/kX+CIiMi8FGvAEBERUerh8ePHY8KECbptb9y4EYsWLcLJkyeRlpaGN998E1OnTsWCBQuK3E5UVBQiIyP11k2ZFoGpn08v9r44OjjC0tKywMk6SqUSTk5Oxd5OSbDNNtvlt53nk86uaOtaFR9siseDx7mCNKV6zMvC8y0GqR5ztqXVJvEI81EdhYiLi8PQoUN1X7/99tuIi4tDSkoKHB0dMXv2bPzwww8v3E54eDjS09P1lrBJ4Ubti5W1NTwaeSL2ZIxunUajQWxsDLy8fYzalrHYZpvt8tsG/h0s+L/mhLHfJSApPcfkvTxSPeZiP99ikeoxZ1tabSHJZMIt5sDoKz2r1WrMnz8f33//PW7fvo3cXP2/lj18+LBY26lWrRqSkpLQoEEDAEBKSgqePXsGOzs7AEDDhg2LtS25vODbj3KeFWsX9AwNHoFpkyfB07MxGjfxwsYN65CdnY2g3n2M3xjbbLPNNoCwLq7o0qg6Pt1+EZm5z1DV1goAkKlSQ/VMY9I2IM1jLnZblZMFZfJd3ddp95Nx7+9rqFTZDg5O1U3aluoxZ1tabRKH0QOGyMhIrFy5Ep988gmmTp2KKVOm4O+//8auXbvw+eefF3s7QUFB+OCDDzBnzhzI5XLMnDkT/v7+qFjx34saXb16FbVq1TJ290qsa2A3pD18iCWLFiI19QHc3D2wZPlKKASYXmObbbbLZ7tvs39fw5YOaaq3fubeP7H3QopJ24A0j7nY7bs3rmJV5ATd1/vWLwYA+PgHoF+IcbPfxpLqMWdbWm2hmMsF1YQi02q1WmPu4OLigoULF6J79+6oUqUKzp8/r1t38uRJbN68uVjbefLkCUaOHIkdO3ZArVbDz88PGzduRP369QEAv/zyC9LT09G/f3+jH1RJZhiIiF6kw9xjorWPTvQXrS1Vey4lidbu4eksWptIKDZG/9laOO9vuyRYa0V/T8FaJWX0U5WcnIwmTZoAACpXroz09HQAQI8ePYy6DkPlypWxdetW5OTk4NmzZ6hcubLe7V26dDF214iIiIiIXhonGPQZfdJz7dq1kZT0719dXFxc8MsvvwAATp8+XaKPMrWxsSkwWCAiIiIiorLB6AFD7969cejQIQDARx99hGnTpqFhw4YYNmwY3n333VLfQSIiIiIiIVnIZIIt5sDotyRFR0fr/v/AgQNRt25d/PHHH2jYsCF69uxZqjtHRERERETieunrMPj6+iI0NBStW7fGl19+WRr7REREREQkmrJ6HYbffvsNPXv2RM2aNSGTybBr164iv//o0aOQyWQFluTkZKO6pXbhtqSkJKNOeiYiIiIiouLLzMyEt7c3Fi9ebNT9rl69iqSkJN1SrVo1o+5fhj/QioiIiIhIeGX1OgyBgYEIDAw0+n7VqlWDg4NDibulNsNARERERETGUalUyMjI0FtUKlWpNpo2bQpnZ2d07twZv//+u9H35wxDKZPqhX7EfNxi4sWVpIUXT5MW/vsmki4h/6IeFRWFyMhIvXURERGYPn36S2/b2dkZy5YtQ4sWLaBSqbBy5Up06NABsbGxaNasWbG3U+wBQ2hoaJG3P3jwoNhRIiIiIiICwsPDC/yeXZJrmxXGzc0Nbm5uuq/btGmDGzduYP78+diwYUOxt1PsAcO5c+de+D3t27cvdpiIiIiIqCwS8hwGuVxeagOE4mjVqhVOnDhh1H2KPWA4cuSI0TtERERERERlx/nz5+HsbNxbLnkOAxERERFRPhZl80OS8OTJE1y/fl33dWJiIs6fP4+qVauiTp06CA8Px927d7F+/XoAwNdff4369evD09MTOTk5WLlyJQ4fPoxffvnFqC4HDEREREREZuDMmTN44403dF/nnfsQHByMtWvXIikpCbdv39bdnpubi08++QR3795FpUqV4OXlhV9//VVvG8Uh02q12tJ5CGVHzjPx2vyUJGnhp6gQERGVjE0Z/rP1+B//FKz1dS93wVolVYafKiIiIiIi4ZXVtySJhRduy2fL5k0I7PwmWvo0wZBB/XEhIUGQbuLleKyPDkf06L6YMqADLp86Lkg3jxQft1SPOdtss80222ybe5uEV6IBw/Hjx/HOO+/Az88Pd+/eBQBs2LDB6I9oAv59b9X333+PCRMmYPDgwRg8eDAmTJiAbdu2ITc3tyS7VyL7f96HubOjMPrDEGzZthNubu4YM3oklEqlydu5qhw413NBz5HjTd56nlQft1SPOdtss80222ybc1soMplMsMUcGD1g2L59OwICAlCxYkWcO3dOd+nq9PR0fPnll0Zt6/r16/Dw8EBwcDDOnTsHjUYDjUaDc+fOYdiwYfD09NQ7E9yUNqxbgz79BiCod1+4uLpiakQkbGxssGvHdpO33Xxao/OgUfBs1c7kredJ9XFL9ZizzTbbbLPNtjm3SRxGDxi++OILLFu2DN9++y2srKx069u2bYu4uDijtjVmzBg0adIEKSkpOHr0KLZu3YqtW7fi6NGjSElJgaenJ0JCQozdRaM9zc3FlcuX4OvXRrfOwsICvr5tkBD/4gvWmSupPm4xiXnM2WabbbbZZtuc20KykAm3mAOjBwxXr14t9IrO9vb2ePTokVHb+v333/HFF1/Azs6uwG12dnaYOXMmjh83/XvL0x6lQa1WQ6FQ6K1XKBRITU01eV8sUn3cYhLzmLPNNttss822ObdJPEYPGGrUqFHo24ROnDiBBg0aGLUtBwcH/P333wZv//vvv+Hg4FDkNlQqFTIyMvSWvLdJEREREREZSyYTbjEHRg8Y3nvvPYwbNw6xsbGQyWS4d+8eNm3ahIkTJ2LMmDFGbWvUqFEYNmwY5s+fj4SEBKSkpCAlJQUJCQmYP38+hg8fjvfff7/IbURFRcHe3l5vmTMryqj9cHRwhKWlZYGTdZRKJZycnIzaljmR6uMWk5jHnG222WabbbbNuU3iMXrA8Nlnn+Htt99Gx44d8eTJE7Rv3x6jRo3C6NGj8dFHHxm1rRkzZmDSpEmYM2cOmjZtipo1a6JmzZpo2rQp5syZg0mTJmH69OlFbiM8PBzp6el6S9ikcKP2w8raGh6NPBF7Mka3TqPRIDY2Bl7ePkZty5xI9XGLScxjzjbbbLPNNtvm3BaShUwm2GIOjL5wm0wmw5QpUxAWFobr16/jyZMnaNSoESpXrlyiHZg0aRImTZqExMREJCcnA/j3bU/169cv1v3lcjnkcrneupJc6Xlo8AhMmzwJnp6N0biJFzZuWIfs7GwE9e5j/MaMpMrJgjL5ru7rtPvJuPf3NVSqbAcHp+ombUv1cUv1mLPNNttss822ObdJHCW+0rO1tTUaNWpUajtSv379AoOEO3fuICIiAqtXry61jiFdA7sh7eFDLFm0EKmpD+Dm7oEly1dCIcD02t0bV7EqcoLu633rFwMAfPwD0C/EuNkSY0n1cUv1mLPNNttss822ObeFwisb65NptVqtMXd44403irzIxOHDh196p/LEx8ejWbNmUKvVRt2vJDMMpWXPpSTR2j08nUVri/m4xSTmMSciIjJnNiX+s7XpTd73l2CtL7u9JlirpIx+qpo2bar39dOnT3H+/HlcvHgRwcHBRm1r9+7dRd5+8+ZNY3ePiIiIiOilmMmpBYIxesAwf/78QtdPnz4dT548MWpbQUFBkMlkKGqSw1wumU1EREREVB6V2lu03nnnHaPPNXB2dsaOHTug0WgKXYy9cjQRERER0cvipyTpK7UBQ0xMDGxsbIy6T/PmzXH27FmDt79o9oGIiIiIiEzL6Lck9emj/5FZWq0WSUlJOHPmDKZNm2bUtsLCwpCZmWnwdldXVxw5csTYXSQiIiIiKjEz+cO/YIweMNjb2+t9bWFhATc3N8yYMQNdunQxalvt2rUr8nZbW1v4+/sbu4tERERERFRKjBowqNVqjBgxAk2aNIGjo6Op9omIiIiISDQWnGHQY9Q5DJaWlujSpQsePXpkot0hIiIiIqKyxOi3JDVu3Bg3b94scFVm+pdUL57GC5gJj883EVH5wNdzKuuM/pSkL774AhMnTsSePXuQlJSEjIwMvYWIiIiIyJzxY1X1FXuGYcaMGfjkk0/QrVs3AMBbb72ld1E1rVYLmUwGtVpd+ntJRERERESiKPaAITIyEh988AE/5pSIiIiIyjUz+cO/YIo9YMi7gBo/5pSIiIiISDqMOulZxuEWEREREZVz/FhVfUYNGF577bUXDhoePnz4UjtERERERERlh1EDhsjIyAJXeiYiIiIiKk9k4BRDfkZ9rOqgQYMQHBxc5GLOtmzehMDOb6KlTxMMGdQfFxISyn078XI81keHI3p0X0wZ0AGXTx0XpJtHisdczDafb7bZZpvt8tGW8us5Ca/YAwYxzl9ISUnBjBkzBGnt/3kf5s6OwugPQ7Bl2064ubljzOiRUCqV5bqdq8qBcz0X9Bw53uSt50n1mPP5ltYxZ5ttttk2Bam+ngvFQibcYg6KPWDI+5QkISUnJyMyMlKQ1oZ1a9Cn3wAE9e4LF1dXTI2IhI2NDXbt2F6u224+rdF50Ch4tmpn8tbzpHrM+XxL65izzTbbbJuCVF/PSRzFHjBoNBpUq1atVOMJCQlFLlevXi3VniFPc3Nx5fIl+Pq10a2zsLCAr28bJMSfK7dtMUn1mPP5ltYxZ5ttttkub6TyuDnDoM+ok55LW9OmTSGTyQqdvchbL8RbodIepUGtVkOhUOitVygUSEy8WW7bYpLqMefzLa1jzjbbbLNd3kj1cUudqAOGqlWrYvbs2ejYsWOht1+6dAk9e/YschsqlQoqlUpvndZSDrlcXmr7SURERETSwWuP6TPqU5JKW/PmzXHv3j3UrVu30KVWrVovPHciKioK9vb2esucWVFG7YejgyMsLS0LnKyjVCrh5ORk9OMyl7aYpHrM+XxL65izzTbbbJc3Un3cUifqgOGDDz5AvXr1DN5ep04drFmzpshthIeHIz09XW8JmxRu1H5YWVvDo5EnYk/G6NZpNBrExsbAy9vHqG0ZS8y2mKR6zPl8S+uYs80222yXN1J53DyHQZ+ob0nq3bt3kbc7Ojq+8NoOcnnBtx/lPDN+X4YGj8C0yZPg6dkYjZt4YeOGdcjOzkZQ7z7Gb8yM2qqcLCiT7+q+TrufjHt/X0OlynZwcKpu0rZUjzmfb2kdc7bZZpttU5Dq6zmJQ9QBw4vcuXMHERERWL16tclbXQO7Ie3hQyxZtBCpqQ/g5u6BJctXQiHA9JqY7bs3rmJV5ATd1/vWLwYA+PgHoF+IcTM1xpLqMefzLa1jzjbbbLNtClJ9PRcKT2HQJ9OKcYGFYoqPj0ezZs2gVquNul9JZhjKgz2XkkRr9/B0Fq0tVXy+iYjKB6m+ntuU4T9bf/WbcJ/4FNq+gWCtkhL1qdq9e3eRt9+8yY/nIiIiIiISk6gDhqCgIIPXYcjDj7UiIiIiIiFZ8PdPPaJ+SpKzszN27NgBjUZT6BIXFyfm7hERERERSZ7o12E4e/aswdtfNPtARERERFTa+LGq+kR9S1JYWBgyMzMN3u7q6oojR44IuEdERERERJSfqAOGdu3aFXm7ra0t/P39BdobIiIiIiJ+rOrzRH1LEhERERERlW1l+BNwiYiIiIiEZwFOMeRXpi/cVlJSvXAbEVF549hyrGjttNOLRGsTSUFZvnDb4t//FqwV0raeYK2SKsNPFRERERGR8HgOgz6ew0BERERERAZxhoGIiIiIKB9zuT6CUDjDQEREREREBnGGgYiIiIgoHwuexKCHMwxERERERGQQZxiIiIiIiPLhBIM+zjDks2XzJgR2fhMtfZpgyKD+uJCQwDbbbLPNthm13+v/Ok5tDUfK8TlIOT4HR9d9gi5tG5m8m5/UjjnbbAvdJuGViQHDP//8gydPnhRY//TpU/z222+C7MP+n/dh7uwojP4wBFu27YSbmzvGjB4JpVLJNttss822mbTvpjzCtG9+RJshs9F2yBwcPfUXts1/Hx4Napi0m0eKx5xttoVsC8VCJhNsMQeiDhiSkpLQqlUr1K1bFw4ODhg2bJjewOHhw4d44403BNmXDevWoE+/AQjq3Rcurq6YGhEJGxsb7NqxnW222WabbTNp7/vtIg6cuIwbtx/g+u37mL74JzzJUqGVV32TdvNI8ZizzbaQbRKHqAOGzz77DBYWFoiNjcX+/ftx+fJlvPHGG0hLS9N9j1arNfl+PM3NxZXLl+Dr10a3zsLCAr6+bZAQf45tttlmm20zaednYSFD/4DmsK1ojdiERJP3pHrM2WZbjH/fpiaTCbeYA1EHDL/++isWLlyIFi1aoFOnTvj999/h7OyMN998Ew8fPgQAyAQ4kmmP0qBWq6FQKPTWKxQKpKamss0222yzbSZtAPB0rYkHv89DeuzXWDhlIAZ+8i3+vJls8q5UjznbbAvVJvGIOmBIT0+Ho6Oj7mu5XI4dO3agXr16eOONN3D//v0XbkOlUiEjI0NvUalUptxtIiIqw/76OwWtB0Wh/bC5+HbbCXw7YyjcBTqHgYioPBJ1wNCgQQMkPHdWfYUKFbBt2zY0aNAAPXr0eOE2oqKiYG9vr7fMmRVl1H44OjjC0tKywMk6SqUSTk5ORm3LWGyzzTbbbJeup8/UuHknFeeu3MHn3+zGhb/uImRwB5N3pXrM2WZbqLaQLARczIGo+xkYGIgVK1YUWJ83aGjatOkLz2EIDw9Henq63hI2Kdyo/bCytoZHI0/EnozRrdNoNIiNjYGXt49R2zIW22yzzTbbpmUhk0FubfrLDkn1mLPNtpj/vkkYol647X//+x+ysrIKva1ChQrYvn077t69W+Q25HI55HK53rqcZ8bvy9DgEZg2eRI8PRujcRMvbNywDtnZ2Qjq3cf4jbHNNttssy1Ke8ZHb+HA75dwJykNVWxtMDCwBdq3aIieHy4xaTePFI8522wL2RaKEOfQmhNRBwwVKlSAnZ2dwduTkpIQGRmJ1atXm3xfugZ2Q9rDh1iyaCFSUx/Azd0DS5avhEKA6TW22WabbbZLxytVK2PVzGGo4WSH9Cc5uHjtLnp+uASHY/80aTePFI8522wL2SZxyLRCfG5pCcXHx6NZs2ZQq9VG3a8kMwxERFT2OLYcK1o77fQi0dpEUmAj6p+ti7b+zB3BWsNavCpYq6REfap2795d5O03b94UaE+IiIiIiKgwog4YgoKCIJPJijyxme8hIyIiIiIhWfD3Tz2ifkqSs7MzduzYAY1GU+gSFxcn5u4REREREUmeqAOG5s2b4+zZswZvf9HsAxERERFRaZMJuJgDUd+SFBYWhszMTIO3u7q64siRIwLuERERERER5SfqgKFdu3ZF3m5rawt/f3+B9oaIiIiICOApDPrM5YrUREREREQkgjL8CbhERERERMLjp3Tq44CBiMzKnktJorV7eDqL1haTmMdczIun8WeNiOhfHDAQEREREeXD9+zr4/EgIiIiIiKDOMNARERERJQPz2HQxxkGIiIiIiIyiAMGIiIiIiIz8Ntvv6Fnz56oWbMmZDIZdu3a9cL7HD16FM2aNYNcLoerqyvWrl1rdJcDBiIiIiKifGQCLsbIzMyEt7c3Fi9eXKzvT0xMRPfu3fHGG2/g/PnzGD9+PEaNGoUDBw4Y1eU5DEREREREZiAwMBCBgYHF/v5ly5ahfv36mDdvHgDAw8MDJ06cwPz58xEQEFDs7XCGIZ8tmzchsPObaOnTBEMG9ceFhAS22Wa7HLQTL8djfXQ4okf3xZQBHXD51HFBunl4zKVzzKX6uNlmW8i2EGQymWCLSqVCRkaG3qJSqUrlccTExKBTp0566wICAhATE2PUdkQfMCiVShw5cgQPHz4EAKSmpmLWrFmYMWMGrly5Ith+7P95H+bOjsLoD0OwZdtOuLm5Y8zokVAqlWyzzbaZt3NVOXCu54KeI8ebvPU8HvPxJm89j49bWj9rbEurXR5FRUXB3t5eb4mKiiqVbScnJ6N69ep666pXr46MjAxkZ2cXezuiDhhOnToFFxcXdOzYEa6urjh79ixatWqFVatWYf369WjevDni4uIE2ZcN69agT78BCOrdFy6urpgaEQkbGxvs2rGdbbbZNvO2m09rdB40Cp6t2pm89Twec2kdc6k+brbZFqotFAsBl/DwcKSnp+st4eHhAj3S4hF1wDBlyhT0798f6enpmDx5MoKCgtCxY0f89ddfuH79OgYNGoSZM2eafD+e5ubiyuVL8PVro1tnYWEBX982SIg/xzbbbJtxW0w85sLj45bWzxrb0mqXV3K5HHZ2dnqLXC4vlW3XqFEDKSkpeutSUlJgZ2eHihUrFns7og4Yzp49i9DQUFSpUgXjxo3DvXv38N577+luHzt2LE6fPm3y/Uh7lAa1Wg2FQqG3XqFQIDU1lW222Tbjtph4zIXHxy2tnzW2pdUWkpDnMJiSn58fDh06pLfu4MGD8PPzM2o7on5KUm5urm50Y2VlhUqVKsHJyUl3u5OT0wvfD6dSqQqcGKK1lJfayIyIiIiIqCx48uQJrl+/rvs6MTER58+fR9WqVVGnTh2Eh4fj7t27WL9+PQDggw8+wKJFi/Dpp5/i3XffxeHDh/H9999j7969RnVFnWF49dVXcfPmTd3XW7ZsgbOzs+7rpKQkvQFEYQo7UWTOLONOFHF0cISlpWWBwYlSqXxh/2WxzTbbpm2LicdceHzc0vpZY1tabSGV1eswnDlzBj4+PvDx8QEAhIaGwsfHB59//jmAf393vn37tu7769evj7179+LgwYPw9vbGvHnzsHLlSqM+UhUQecAwaNAg3L9/X/d19+7d9d5PtXv3brRq1arIbRR2okjYJONOFLGytoZHI0/Envz/j5jSaDSIjY2Bl7ePUdsyFttss23atph4zIXHxy2tnzW2pdUmoEOHDtBqtQWWvKs3r127FkePHi1wn3PnzkGlUuHGjRsYPny40V1R35IUERFR5O1TpkyBpaVlkd8jlxd8+1HOM+P3ZWjwCEybPAmeno3RuIkXNm5Yh+zsbAT17mP8xthmm+0y1VblZEGZfFf3ddr9ZNz7+xoqVbaDg1P1Iu758njM/yWVYy7Vx80220K1hWLiUwvMTpm+0rNSqURERARWr15t8lbXwG5Ie/gQSxYtRGrqA7i5e2DJ8pVQCDC9xjbbbJvW3RtXsSpygu7rfesXAwB8/APQL8S0H13HY/4vqRxzqT5uttkWqk3ikGm1Wq3YO2FIfHw8mjVrBrVabdT9SjLDQETmYc+lJNHaPTydX/xN5ZBUj7lUHzeRUGzK8J+tf7qQ8uJvKiU9m5h29rE0iPpU7d69u8jb858QTUREREREwhN1wBAUFASZTIaiJjlM/fm0RERERET58ddPfaJ+SpKzszN27NgBjUZT6BIXFyfm7hERERERSZ6oA4bmzZvj7NmzBm9/0ewDEREREVFpkwn4P3Mg6luSwsLCkJmZafB2V1dXHDlyRMA9IiIiIiKi/EQdMLRr167I221tbeHv7y/Q3hARERER8RyG54n6liQiIiIiIirbOGAgIiIiIiKDyvAlM4iICuIFrYTHYy68DnOPidY+OpFvBSayMJOTkYXCGQYiIiIiIjKIMwxERERERPnwpGd9nGEgIiIiIiKDOMNARERERJQPZxj0cYaBiIiIiIgM4gwDEREREVE+Mn5Kkh7OMBARERERkUEcMOSzZfMmBHZ+Ey19mmDIoP64kJDANttss80228WWeDke66PDET26L6YM6IDLp44L0h3m+ypWB/vg0IS22PeRH2b18USdqhUFaeeR4vPNtjhtIVjIhFvMQZkcMDRo0ADXrl0TtLn/532YOzsKoz8MwZZtO+Hm5o4xo0dCqVSyzTbbbLPNdrHkqnLgXM8FPUeON3krP586Dtgedw+jNpzDx1sTUMFChgUDvWBjJcx/5qX6fLMtfJvEIdNqtVqx4gsXLix0fWhoKD799FPUqFEDAPDxxx8btd2cZ8bvy5BB/eHZuAkmT/0cAKDRaNCloz8Gvz0UI9973/gNss0222yzbdbtPZeSXmo/pgzogCETZ6JRq3ZG33fuz3+9VNuhohX2j2uDDzadx/k76UbdtyRXei4PzzfbwrdtyvCZtIf/FG7w86a7QrBWSYk6wzB+/HjMmTMH8+fP11s0Gg3Wr1+P+fPn4+uvvzb5fjzNzcWVy5fg69dGt87CwgK+vm2QEH+ObbbZZpttts1KZbklACAj+6nJW1J9vtmW9r8xqRF1wPD+++/DyckJ+/btQ2Jiom6xtLTEL7/8gsTERNy8ebPIbahUKmRkZOgtKpXKqP1Ie5QGtVoNhUJ/hKdQKJCammr042KbbbbZZlt67bJCBmB8J1fE30nHzdQsk/ek+nyzXb7/jclkwi3mQNQBw7Jly/D5558jICAAixYtKtE2oqKiYG9vr7fMmRVVyntKRERkHsK6NITLK7aYuvuy2LtCROWE6Cc99+7dGzExMdi5cycCAwORnJxs1P3Dw8ORnp6ut4RNCjdqG44OjrC0tCxwso5SqYSTk5NR2zIW22yzzTbb5aNdFnzS2RVtXaviw83xePA4V5CmVJ9vtsv3vzGZgP8zB6IPGACgVq1a+PXXX9G+fXv4+PjAmPOw5XI57Ozs9Ba5XG5U38raGh6NPBF7Mka3TqPRIDY2Bl7ePkZty1hss80222yXj7bYPunsCv/XnDD2uwQkpecI1pXq88229P6NSVmZOT9dJpMhPDwcXbp0wYkTJ+Ds7Cxof2jwCEybPAmeno3RuIkXNm5Yh+zsbAT17sM222yzzTbbxaLKyYIy+a7u67T7ybj39zVUqmwHB6fqJuuGdXFFl0bV8en2i8jMfYaqtlYAgEyVGqpnGpN180j1+WZb+LZQzOX6CEIpMwOGPM2bN0fz5s0BAHfu3EFERARWr15t8m7XwG5Ie/gQSxYtRGrqA7i5e2DJ8pVQCDC9xjbbbLPNdvlo371xFasiJ+i+3rd+MQDAxz8A/UKMe7usMfo2qwUAWDqkqd76mXv/xN4LKSbr5pHq88228G0Sh6jXYXiR+Ph4NGvWDGq12qj7leQ6DERERPm97HUYXsbLXofhZZTkOgxEJVGWr8Pw218PBWu1f62qYK2SEvWp2r17d5G3v+gjVYmIiIiISpu5nIwsFFEHDEFBQZDJZEWe5Cwzlw+oJSIiIiIqh0T9lCRnZ2fs2LEDGo2m0CUuLk7M3SMiIiIiCeKF2/SJOmBo3rw5zp49a/D2F80+EBERERGRaYn6lqSwsDBkZmYavN3V1RVHjhwRcI+IiIiISOrM5A//ghF1wNCuXbsib7e1tYW/Pz+tgYiIiIhILGX4A62IiIiIiIRnYS4nFwhE1HMYiIiIiIiobOMMAxERFUnMC5j18HSWZFuqpPqzRmUP5xf0cYaBiIiIiIgM4gwDEREREVF+nGLQwxkGIiIiIiIyiDMMRERERET5yDjFoIczDEREREREZBBnGIiIiIiI8uFlGPRxhoGIiIiIiAzigCGfLZs3IbDzm2jp0wRDBvXHhYQEttlmm222SyDxcjzWR4cjenRfTBnQAZdPHRekm4fHXDrHXKqPW8ptIcgEXMxBmRowaLVaHDlyBN9++y327NmDp0+fCtbe//M+zJ0dhdEfhmDLtp1wc3PHmNEjoVQq2WabbbbZNlKuKgfO9VzQc+R4k7eex2M+3uSt5/FxS+tnTcw2iUPUAUO3bt2Qnp4OAHj48CH8/PzQsWNHTJkyBb169YKXlxcePHggyL5sWLcGffoNQFDvvnBxdcXUiEjY2Nhg147tbLPNNttsG8nNpzU6DxoFz1btTN56Ho+5tI65VB+3VNuC4RSDHlEHDPv374dKpQIATJ06FY8fP8aNGzdw//593Lp1C7a2tvj8889Nvh9Pc3Nx5fIl+Pq10a2zsLCAr28bJMSfY5ttttlm20zwmAuPj1taP2tSfb6lrsy8Jenw4cOIiopC/fr1AQC1a9fGrFmzcODAgSLvp1KpkJGRobfkDUKKK+1RGtRqNRQKhd56hUKB1NRU4x6Ikdhmm222y1tbTDzmwuPjltbPmlSfb6kTfcAg++9zq9LS0uDi4qJ3m6urK+7du1fk/aOiomBvb6+3zJkVZbL9JSIiIqLyTSbg/8yB6NdhGD58OORyOZ4+fYrExER4enrqbktOToaDg0OR9w8PD0doaKjeOq2l3Kh9cHRwhKWlZYGTdZRKJZycnIzalrHYZptttstbW0w85sLj45bWz5pUn2+pE3WGITg4GNWqVYO9vT169eqFrKwsvdu3b9+Opk2bFrkNuVwOOzs7vUUuN27AYGVtDY9Gnog9GaNbp9FoEBsbAy9vH6O2ZSy22Wab7fLWFhOPufD4uKX1syaV51smE24xB6LOMKxZs6bI2yMiImBpaSnIvgwNHoFpkyfB07MxGjfxwsYN65CdnY2g3n3YZpttttk2kionC8rku7qv0+4n497f11Cpsh0cnKqbtM1j/i+pHHOpPm6ptkkcor8lqSgPHz5EREQEVq9ebfJW18BuSHv4EEsWLURq6gO4uXtgyfKVUAgwvcY222yzXd7ad29cxarICbqv961fDADw8Q9Av5Bwk7Z5zP8llWMu1cct1bZQzOQP/4KRabVardg7YUh8fDyaNWsGtVpt1P1ynploh4iIJGjPpSTR2j08nUVri0mqx1yqj1uqbMrwn63j/s4QrNWsnp1grZIS9anavXt3kbffvHlToD0hIiIiIvoPpxj0iDpgCAoKgkwmQ1GTHDJzORuEiIiIiKgcEvVTkpydnbFjxw5oNJpCl7i4ODF3j4iIiIgkiNdh0CfqgKF58+Y4e/aswdtfNPtARERERESmJepbksLCwpCZmWnwdldXVxw5ckTAPSIiIiIiqeM74vWJOmBo165dkbfb2trC399foL0hIiIiIqLnleEPtCIiIiIiEh4nGPSJeg4DERERERGVbWX6wm0lxQu3CY8X2yEiU+BrCwmFP2vCK8sXbou/81iwlverVQRrlRRnGIiIiIiIyKAyPLYjIiIiIhKeuVwfQSicYSAiIiIiIoM4YCAiIiIiIoP4liQiIiIionx44TZ9nGEgIiIiIiKDOMNARERERJQPJxj0cYYhny2bNyGw85to6dMEQwb1x4WEBLZNKPFyPNZHhyN6dF9MGdABl08dF6SbR4rHnG22pdDmawvb/Fkr320SnqgDhn/++Qepqam6r48fP44hQ4agXbt2eOeddxATEyPYvuz/eR/mzo7C6A9DsGXbTri5uWPM6JFQKpVsm0iuKgfO9VzQc+R4k7eeJ9VjzjbbUmjztYVt/qyV37ZgZAIuZkDUAUPfvn1x8uRJAMCPP/6IDh064MmTJ2jbti2ysrLg7++PPXv2CLIvG9atQZ9+AxDUuy9cXF0xNSISNjY22LVjO9sm4ubTGp0HjYJnq3Ymbz1Pqsecbbal0OZrC9v8WSu/bRKHqAOGS5cuwdPTEwAQFRWFL7/8Ej/++COio6OxY8cOfPXVV/j8889Nvh9Pc3Nx5fIl+Pq10a2zsLCAr28bJMSfY7uckeoxZ5ttKbTFJNVjLtW2mHjMTU8m4P/MgagDhgoVKuDx48cAgMTERAQGBurdHhgYiKtXrxa5DZVKhYyMDL1FpVIZtR9pj9KgVquhUCj01isUCr23TJmCVNtikuoxZ5ttKbTFJNVjLtW2mHjMSWiiDhj8/f3x3XffAQB8fHxw9OhRvduPHDmCWrVqFbmNqKgo2Nvb6y1zZkWZapeJiIiIqJyTyYRbzIGoH6saHR2Ndu3a4d69e3j99dcxZcoUnD59Gh4eHrh69Sq2bt2KZcuWFbmN8PBwhIaG6q3TWsqN2g9HB0dYWloWOFlHqVTCycnJqG0ZS6ptMUn1mLPNthTaYpLqMZdqW0w85rR48WLMmTMHycnJ8Pb2xjfffINWrVoV+r1r167FiBEj9NbJ5XLk5OQUuyfqDIOHhwdiY2ORm5uL2bNnIzMzE5s2bcL06dNx/fp1bNmyBcOHDy9yG3K5HHZ2dnqLXG7cgMHK2hoejTwRe/L/P5VJo9EgNjYGXt4+JXlobJdhUj3mbLMthbaYpHrMpdoWE4+56ZXlD0naunUrQkNDERERgbi4OHh7eyMgIAD37983eB87OzskJSXpllu3bhnVFP3CbS4uLvjuu++g1Wpx//59aDQaODk5wcrKStD9GBo8AtMmT4KnZ2M0buKFjRvWITs7G0G9+7BtIqqcLCiT7+q+TrufjHt/X0OlynZwcKpu0rZUjznbbEuhzdcWtvmzZlpitgn46quv8N577+lmDZYtW4a9e/di9erV+Oyzzwq9j0wmQ40aNUrcFH3AkEcmk6F6df1/XHfu3EFERARWr15t8n7XwG5Ie/gQSxYtRGrqA7i5e2DJ8pVQCDC9JtX23RtXsSpygu7rfesXAwB8/APQLyTcpG2pHnO22ZZCm68tbPNnzbTEbAtGwHMLVCpVgQ/skcvlhb5jJjc3F2fPnkV4+P//fFlYWKBTp05FXr/syZMnqFu3LjQaDZo1a4Yvv/xS90mlxSHTarXaYn+3wOLj49GsWTOo1Wqj7pfzzEQ7RAbtuZQkWruHp7NobSIyLb62kFD4syY8mzLzZ+uCriRlCtbaunwOIiMj9dZFRERg+vTpBb733r17qFWrFv744w/4+fnp1n/66ac4duwYYmNjC9wnJiYG165dg5eXF9LT0zF37lz89ttvuHTpEmrXrl2sfRT1qdq9e3eRt9+8eVOgPSEiIiIi+peQ10co7AN8jD0ftyh+fn56g4s2bdrAw8MDy5cvx8yZM4u1DVEHDEFBQZDJZChqkkNmLp83RURERERkJENvPyqMk5MTLC0tkZKSorc+JSWl2OcoWFlZwcfHB9evXy/2Por6KUnOzs7YsWMHNBpNoUtcXJyYu0dEREREElRWr8NgbW2N5s2b49ChQ7p1Go0Ghw4d0ptFKIparcaFCxfg7Fz8t8KJOmBo3rw5zp49a/D2F80+EBERERFJSWhoKL799lusW7cOV65cwZgxY5CZman71KRhw4bpnRQ9Y8YM/PLLL7h58ybi4uLwzjvv4NatWxg1alSxm6K+JSksLAyZmYZPKnF1dcWRI0cE3CMiIiIiorJr4MCBePDgAT7//HMkJyejadOm2L9/v+7TRm/fvg0Li/+fE0hLS8N7772H5ORkODo6onnz5vjjjz/QqFGjYjfL9KcklRQ/JUl4/HQJIjIFvraQUPizJryy/ClJfyVnCdZ6rUYlwVolJepbkoiIiIiIqGwrw2M7IiIiIiIR8EM69XDAUMo4pUlEVHr4ukZEJD4OGIiIiIiI8hHywm3mgOcwEBERERGRQZxhICIiIiLKx9gLqpV3nGEgIiIiIiKDOMNARERERJQPJxj0cYaBiIiIiIgM4gwDEREREVF+nGLQwxmGfLZs3oTAzm+ipU8TDBnUHxcSEgTpJl6Ox/rocESP7ospAzrg8qnjgnTz8HEL+7jZZpttttkuH23+d0ycNglP1AHDvHnzcOvWLTF3QWf/z/swd3YURn8Ygi3bdsLNzR1jRo+EUqk0eTtXlQPnei7oOXK8yVvP4+MW/nGzzTbbbLNdPtr875jwbaHIBPyfORB1wBAWFgYXFxd07twZW7duRW5urmj7smHdGvTpNwBBvfvCxdUVUyMiYWNjg107tpu87ebTGp0HjYJnq3Ymbz2Pj1v4x80222yzzXb5aPO/Y8K3SRyivyVp5cqVsLW1xdChQ1GzZk2MHz8eFy9eFHQfnubm4srlS/D1a6NbZ2FhAV/fNkiIPyfovgiJj1v4x80222yzzXb5aIuJx9z0ZDLhFnMg+oChW7du2LVrF/755x98+umnOHDgALy9vdGqVSt8++23ePz4cZH3V6lUyMjI0FtUKpVR+5D2KA1qtRoKhUJvvUKhQGpqqtGPyVzwcQv/uNlmm2222S4fbTHxmJPQRB8w5KlWrRo+/fRTXLlyBUePHkWjRo0wYcIEODs7F3m/qKgo2Nvb6y1zZkUJtNdEREREVN7IBFzMgagfqyozMA/Trl07tGvXDgsXLsTWrVuL3EZ4eDhCQ0P11mkt5Ubth6ODIywtLQucrKNUKuHk5GTUtswJH7fwj5ttttlmm+3y0RYTjzkJTdQZBq1WW+TtdnZ2eO+994r8HrlcDjs7O71FLjduwGBlbQ2PRp6IPRmjW6fRaBAbGwMvbx+jtmVO+LiFf9xss80222yXj7aYeMwFwCkGPaLOMGg0GjHzeoYGj8C0yZPg6dkYjZt4YeOGdcjOzkZQ7z4mb6tysqBMvqv7Ou1+Mu79fQ2VKtvBwam6Sdt83MI/brbZZptttstHm/8dE75N4ijTV3q+c+cOIiIisHr1apO3ugZ2Q9rDh1iyaCFSUx/Azd0DS5avhEKA6bW7N65iVeQE3df71i8GAPj4B6BfSLhJ23zcwj9uttlmm222y0eb/x0Tvk3ikGlf9L4gEcXHx6NZs2ZQq9VG3S/nmYl2qBj2XEoSrd3Ds+gTxE1Jqo+biIjKB/53THg2ZfjP1reUxn3i5suoqzDurfRiEPWp2r17d5G337x5U6A9ISIiIiKiwog6YAgKCoJMJivy5GdDn6RERERERGQK/PVTn6ifkuTs7IwdO3ZAo9EUusTFxYm5e0REREREkifqgKF58+Y4e/aswdtfNPtARERERFTa+Kmq+kR9S1JYWBgyMzMN3u7q6oojR44IuEdERERERJSfqAOGdu3aFXm7ra0t/P39BdobIiIiIiKew/A8Ud+SREREREREZVsZ/gRcIiIiIiIxcIohvzJ94baSEvPCbURUfvHCTkRkCh3mHhOtfXSieG/9LssXbvsnLVewVm1Ha8FaJVWGnyoiIiIiIuHxHAZ9PIeBiIiIiIgM4gwDEREREVE+nGDQxxkGIiIiIiIyiDMMRERERET58BwGfZxhICIiIiIigzjDQERERESUj4xnMejhDEM+WzZvQmDnN9HSpwmGDOqPCwkJbLPNNtsvJfFyPNZHhyN6dF9MGdABl08dF6SbR4rHnG22y3t7mO+rWB3sg0MT2mLfR36Y1ccTdapWNHk3PzGPOQlP9AHDnj178Pnnn+P3338HABw+fBjdunVD165dsWLFCsH2Y//P+zB3dhRGfxiCLdt2ws3NHWNGj4RSqWSbbbbZLrFcVQ6c67mg58jxJm89T6rHnG22y3vbp44Dtsfdw6gN5/Dx1gRUsJBhwUAv2FgJ82udmMecxCHqgGH58uXo3bs39u3bh27dumHjxo0ICgpCrVq1UK9ePYwfPx4LFiwQZF82rFuDPv0GIKh3X7i4umJqRCRsbGywa8d2ttlmm+0Sc/Npjc6DRsGzVTuTt54n1WPONtvlvT3h+wvYeyEFialZuH4/EzP3XoWzvQ3ca1QxaTePmMdcMDIBFzMg6oBh4cKFWLJkCc6cOYNdu3bhvffeQ3R0NL799lssW7YMS5YswfLly02+H09zc3Hl8iX4+rXRrbOwsICvbxskxJ9jm2222TY7Uj3mbLMthfbzKsstAQAZ2U9N3ipLj5uEI+qAITExEQEBAQCAN954A2q1Gu3bt9fd3qFDB9y6davIbahUKmRkZOgtKpXKqP1Ie5QGtVoNhUKht16hUCA1NdWobRmLbbbZLr9tMUn1mLPNthTa+ckAjO/kivg76biZmmXyXll53KbGCQZ9og4YFAqFbkBw7949PHv2DLdv39bdfuvWLVStWrXIbURFRcHe3l5vmTMryqT7TURERFQWhHVpCJdXbDF192Wxd4XKMVE/VrVXr14YOXIkgoODsXv3bgwbNgyffPIJLCwsIJPJEBYWhi5duhS5jfDwcISGhuqt01rKjdoPRwdHWFpaFjhZR6lUwsnJyahtGYttttkuv20xSfWYs822FNp5PunsirauVfHBpng8eJwrSLMsPG4h8MJt+kSdYZg1axY6dOiALVu2oGnTplixYgVGjhyJXr16ITAwEAqFAlFRRc8WyOVy2NnZ6S1yuXEDBitra3g08kTsyRjdOo1Gg9jYGHh5+5TosbHNNttsi0mqx5xttqXQBv4dLPi/5oSx3yUgKT3H5L08Yj9uEoeoMwy2trYFPjp14sSJGDt2LJ4+fYoqVYQ52x8AhgaPwLTJk+Dp2RiNm3hh44Z1yM7ORlDvPmyzzTbbJabKyYIy+a7u67T7ybj39zVUqmwHB6fqJm1L9ZizzXZ5b4d1cUWXRtXx6faLyMx9hqq2VgCATJUaqmcak7YBcY+5UHjhNn1l8krPNjY2sLGxwZ07dxAREYHVq1ebvNk1sBvSHj7EkkULkZr6AG7uHliyfCUUAkyvsc022+W3fffGVayKnKD7et/6xQAAH/8A9AsJN2lbqsecbbbLe7tvs1oAgKVDmuqtn7n3T+y9kGLSNiDuMSdxyLRarVbsnTAkPj4ezZo1g1qtNup+Oc9MtENEJGl7LiWJ1u7h6Sxam4hMq8PcY6K1j070F61tUyb/bP2vB0+E+2Xylcpl+ED8R9Q93L17d5G337x5U6A9ISIiIiKiwog6YAgKCoJMJkNRkxwynqZORERERALib5/6RP2UJGdnZ+zYsQMajabQJS4uTszdIyIiIiKSPFEHDM2bN8fZs2cN3v6i2QciIiIiotImkwm3mANR35IUFhaGzMxMg7e7urriyJEjAu4RERERERHlJ+qAoV27dkXebmtrC39/8c7eJyIiIiLp4XUY9In6liQiIiIiIirbyv4HvxIRERERCchczi0QSpm+cFtJiXnhNjEv7CQmXlSKiIjI/Ij5e0s/77L7u0NalnEXDX4ZjpUsBWuVFN+SREREREREBnHAQEREREREBnHAQEREREREBvGkZyIiIiKifHjSsz7OMBARERERkUGcYSAiIiIiyocXbtPHGQYiIiIiIjKIA4Z8tmzehMDOb6KlTxMMGdQfFxISBOkmXo7H+uhwRI/uiykDOuDyqeOCdMVuA+Idc7bZZpttttlmu2TE/t1BCDKZcIs5EH3AkJ2djdWrV+Pdd99FYGAgunfvjo8++giHDh0SdD/2/7wPc2dHYfSHIdiybSfc3NwxZvRIKJVKk7dzVTlwrueCniPHm7xVltpiHnO22WabbbbZZrtkxPzdgcQh6oDh+vXr8PDwQHh4OH799VccOHAAMpkMp0+fRkBAAAYMGIBnz4S5bPOGdWvQp98ABPXuCxdXV0yNiISNjQ127dhu8rabT2t0HjQKnq3ambxVltpiHnO22WabbbbZZrtkxPzdQSgyARdzIOqA4eOPP0bXrl2RnJyM27dvIyoqChqNBidPnsSVK1dw+vRpfPHFFybfj6e5ubhy+RJ8/dro1llYWMDXtw0S4s+ZvC9FYh5zttlmm2222WabqPhEHTAcO3YMn3zyCWT/vYFrwoQJ+PXXX6FUKtGwYUN8/fXXWLduXZHbUKlUyMjI0FtUKpVR+5H2KA1qtRoKhUJvvUKhQGpqqnEPiopFzGPONttss80222xTkTjFoEfUAYODgwMeP36s+zorKwvPnj2DtbU1AMDLywtJSUlFbiMqKgr29vZ6y5xZUSbdbyIiIiIiqRD1OgydO3dGaGgoli1bBrlcjvDwcDRt2hRVqlQBANy+fRvVqlUrchvh4eEIDQ3VW6e1lBu1H44OjrC0tCxwopBSqYSTk5NR26LiEfOYs80222yzzTbbVBReh0GfqDMMs2fPhkqlQqNGjeDq6oqTJ09i1apVutsfPHiAsLCwIrchl8thZ2ent8jlxg0YrKyt4dHIE7EnY3TrNBoNYmNj4OXtY9yDomIR85izzTbbbLPNNttExSfqDEO1atUQExODa9euQaVSwd3dHRUq/P8u9evXT7B9GRo8AtMmT4KnZ2M0buKFjRvWITs7G0G9+5i8rcrJgjL5ru7rtPvJuPf3NVSqbAcHp+rlti3mMWebbbbZZptttktGzN8dhGIu10cQiqgDhjwNGzYsdP2dO3cQERGB1atXm3wfugZ2Q9rDh1iyaCFSUx/Azd0DS5avhEKAqb27N65iVeQE3df71i8GAPj4B6BfSHi5bYt5zNlmm2222Wab7ZIR83cHEodMq9Vqxd4JQ+Lj49GsWTOo1Wqj7pcjzKUbCrXnUtEnaZdXPTydxd4FIiIiMpKYv7f08y67vztk5Qr363El67I/nSHqDMPu3buLvP3mzZsC7QkRERERERVG1AFDUFAQZDIZiprkkPFNZEREREQkJP76qUfUT0lydnbGjh07oNFoCl3i4uLE3D0iIiIiIskTdcDQvHlznD171uDtL5p9ICIiIiIi0xJ1wBAWFoY2bdoYvN3V1RVHjhwRcI+IiIiISOpkAv6vJBYvXox69erBxsYGrVu3xqlTp4r8/m3btsHd3R02NjZo0qQJ9u3bZ1RP1AFDu3bt0LVrV4O329rawt/fX8A9IiIiIiIqu7Zu3YrQ0FBEREQgLi4O3t7eCAgIwP379wv9/j/++AODBw/GyJEjce7cOQQFBSEoKAgXL14sdrNMf6xqSfFjVYXHj1UlIiIyP/xY1cIJ+bukjZEfQdS6dWu0bNkSixYtAvDvVb5fffVVfPTRR/jss88KfP/AgQORmZmJPXv26Nb5+vqiadOmWLZsWbGaos4wEBERERFJmUqlQkZGht6iUqkK/d7c3FycPXsWnTp10q2zsLBAp06dEBMTU+h9YmJi9L4fAAICAgx+f6G0pCcnJ0cbERGhzcnJYZttttlmm2222Wa7DLbLk4iICC0AvSUiIqLQ7717964WgPaPP/7QWx8WFqZt1apVofexsrLSbt68WW/d4sWLtdWqVSv2PpbLtyS9jIyMDNjb2yM9PR12dnZss80222yzzTbbbJexdnmiUqkKzCjI5XLI5fIC33vv3j3UqlULf/zxB/z8/HTrP/30Uxw7dgyxsbEF7mNtbY1169Zh8ODBunVLlixBZGQkUlJSirWPol64jYiIiIhIygwNDgrj5OQES0vLAr/op6SkoEaNGoXep0aNGkZ9f2F4DgMRERERkRmwtrZG8+bNcejQId06jUaDQ4cO6c045Ofn56f3/QBw8OBBg99fGM4wEBERERGZidDQUAQHB6NFixZo1aoVvv76a2RmZmLEiBEAgGHDhqFWrVqIiooCAIwbNw7+/v6YN28eunfvji1btuDMmTNYsWJFsZscMDxHLpcjIiKi2FNDbLPNNttss80222yTUAYOHIgHDx7g888/R3JyMpo2bYr9+/ejevXqAIDbt2/DwuL/30TUpk0bbN68GVOnTsXkyZPRsGFD7Nq1C40bNy52kyc9ExERERGRQTyHgYiIiIiIDOKAgYiIiIiIDOKAgYiIiIiIDOKAgYiIiIiIDOKAIZ/FixejXr16sLGxQevWrXHq1ClBur/99ht69uyJmjVrQiaTYdeuXYJ0o6Ki0LJlS1SpUgXVqlVDUFAQrl69Kkh76dKl8PLygp2dHezs7ODn54eff/5ZkPbzoqOjIZPJMH78eJO3pk+fDplMpre4u7ubvJvn7t27eOedd6BQKFCxYkU0adIEZ86cEaRdr169Ao9dJpMhJCTEpF21Wo1p06ahfv36qFixIlxcXDBz5kwI9XkPjx8/xvjx41G3bl1UrFgRbdq0wenTp03SetFriVarxeeffw5nZ2dUrFgRnTp1wrVr10ze3bFjB7p06QKFQgGZTIbz58+/dLO4/adPn2LSpElo0qQJbG1tUbNmTQwbNgz37t0zeRv499+8u7s7bG1t4ejoiE6dOhV6JVZTtPP74IMPIJPJ8PXXXwvSHj58eIF/6127djV5FwCuXLmCt956C/b29rC1tUXLli1x+/btl24Xp1/Ya5xMJsOcOXNM2n3y5AnGjh2L2rVro2LFimjUqBGWLVv2Us3itlNSUjB8+HDUrFkTlSpVQteuXUvldYXKFg4Y/rN161aEhoYiIiICcXFx8Pb2RkBAAO7fv2/ydmZmJry9vbF48WKTt/I7duwYQkJCcPLkSRw8eBBPnz5Fly5dkJmZafJ27dq1ER0djbNnz+LMmTN488030atXL1y6dMnk7fxOnz6N5cuXw8vLS7Cmp6cnkpKSdMuJEycE6aalpaFt27awsrLCzz//jMuXL2PevHlwdHQUpH/69Gm9x33w4EEAQP/+/U3anTVrFpYuXYpFixbhypUrmDVrFmbPno1vvvnGpN08o0aNwsGDB7FhwwZcuHABXbp0QadOnXD37t1Sb73otWT27NlYuHAhli1bhtjYWNja2iIgIAA5OTkm7WZmZuL111/HrFmzXqpTkn5WVhbi4uIwbdo0xMXFYceOHbh69Sreeustk7cB4LXXXsOiRYtw4cIFnDhxAvXq1UOXLl3w4MEDk7fz7Ny5EydPnkTNmjVfumlMu2vXrnr/5r/77juTd2/cuIHXX38d7u7uOHr0KBISEjBt2jTY2Ni8dLs4/fyPNykpCatXr4ZMJkPfvn1N2g0NDcX+/fuxceNGXLlyBePHj8fYsWOxe/ful+q+qK3VahEUFISbN2/ixx9/xLlz51C3bl106tRJkN8lSEBa0mq1Wm2rVq20ISEhuq/VarW2Zs2a2qioKEH3A4B2586dgjbz3L9/XwtAe+zYMVH6jo6O2pUrVwrWe/z4sbZhw4bagwcPav39/bXjxo0zeTMiIkLr7e1t8k5hJk2apH399ddFaRdm3LhxWhcXF61GozFpp3v37tp3331Xb12fPn20Q4YMMWlXq9Vqs7KytJaWlto9e/borW/WrJl2ypQpJm0//1qi0Wi0NWrU0M6ZM0e37tGjR1q5XK797rvvTNbNLzExUQtAe+7cuVLrGdPPc+rUKS0A7a1btwRvp6enawFof/31V0Ha//zzj7ZWrVraixcvauvWraudP39+qXYNtYODg7W9evUq9daLugMHDtS+8847Ju0W1X9er169tG+++abJu56entoZM2borTPF68zz7atXr2oBaC9evKhbp1arta+88or222+/LdU2iYszDAByc3Nx9uxZdOrUSbfOwsICnTp1QkxMjIh7Jqz09HQAQNWqVQXtqtVqbNmyBZmZmUZdpvxlhYSEoHv37nrPuxCuXbuGmjVrokGDBhgyZEipTZW/yO7du9GiRQv0798f1apVg4+PD7799ltB2s/Lzc3Fxo0b8e6770Imk5m01aZNGxw6dAh//fUXACA+Ph4nTpxAYGCgSbsA8OzZM6jV6gJ/3axYsaJgM0t5EhMTkZycrPfzbm9vj9atW0vqdQ7497VOJpPBwcFB0G5ubi5WrFgBe3t7eHt7m7yn0WgwdOhQhIWFwdPT0+S95x09ehTVqlWDm5sbxowZA6VSadKeRqPB3r178dprryEgIADVqlVD69atBXub7/NSUlKwd+9ejBw50uStNm3aYPfu3bh79y60Wi2OHDmCv/76C126dDFpV6VSAYDea5yFhQXkcrngr3FkWhwwAEhNTYVardZdIS9P9erVkZycLNJeCUuj0WD8+PFo27atUVf+exkXLlxA5cqVIZfL8cEHH2Dnzp1o1KiRIO0tW7YgLi5Od9l0obRu3Rpr167F/v37sXTpUiQmJqJdu3Z4/Pixyds3b97E0qVL0bBhQxw4cABjxozBxx9/jHXr1pm8/bxdu3bh0aNHGD58uMlbn332GQYNGgR3d3dYWVnBx8cH48ePx5AhQ0zerlKlCvz8/DBz5kzcu3cParUaGzduRExMDJKSkkzezy/vtUzKr3MAkJOTg0mTJmHw4MGws7MTpLlnzx5UrlwZNjY2mD9/Pg4ePAgnJyeTd2fNmoUKFSrg448/NnnreV27dsX69etx6NAhzJo1C8eOHUNgYCDUarXJmvfv38eTJ08QHR2Nrl274pdffkHv3r3Rp08fHDt2zGRdQ9atW4cqVaqgT58+Jm998803aNSoEWrXrg1ra2t07doVixcvRvv27U3adXd3R506dRAeHo60tDTk5uZi1qxZ+OeffwR/jSPTqiD2DlDZEBISgosXLwr6FwE3NzecP38e6enp+OGHHxAcHIxjx46ZfNBw584djBs3DgcPHiy197UWV/6/ant5eaF169aoW7cuvv/+e5P/FUqj0aBFixb48ssvAQA+Pj64ePEili1bhuDgYJO2n7dq1SoEBgaW6nuqDfn++++xadMmbN68GZ6enjh//jzGjx+PmjVrCvK4N2zYgHfffRe1atWCpaUlmjVrhsGDB+Ps2bMmb5O+p0+fYsCAAdBqtVi6dKlg3TfeeAPnz59Hamoqvv32WwwYMACxsbGoVq2ayZpnz57FggULEBcXZ/JZvMIMGjRI9/+bNGkCLy8vuLi44OjRo+jYsaNJmhqNBgDQq1cvTJgwAQDQtGlT/PHHH1i2bBn8/f1N0jVk9erVGDJkiCD/nfnmm29w8uRJ7N69G3Xr1sVvv/2GkJAQ1KxZ06Sz6FZWVtixYwdGjhyJqlWrwtLSEp06dUJgYKBgHyxBwuAMAwAnJydYWloiJSVFb31KSgpq1Kgh0l4JZ+zYsdizZw+OHDmC2rVrC9a1traGq6srmjdvjqioKHh7e2PBggUm7549exb3799Hs2bNUKFCBVSoUAHHjh3DwoULUaFCBZP+Bex5Dg4OeO2113D9+nWTt5ydnQsMxjw8PAR7S1SeW7du4ddff8WoUaME6YWFhelmGZo0aYKhQ4diwoQJgs0uubi44NixY3jy5Anu3LmDU6dO4enTp2jQoIEg/Tx5r2VSfZ3LGyzcunULBw8eFGx2AQBsbW3h6uoKX19frFq1ChUqVMCqVatM2jx+/Dju37+POnXq6F7nbt26hU8++QT16tUzabswDRo0gJOTk0lf65ycnFChQoUy8Tp3/PhxXL16VZDXuezsbEyePBlfffUVevbsCS8vL4wdOxYDBw7E3LlzTd5v3rw5zp8/j0ePHiEpKQn79++HUqkU/DWOTIsDBvz7i2vz5s1x6NAh3TqNRoNDhw4J+p56oWm1WowdOxY7d+7E4cOHUb9+fVH3R6PR6N4PaUodO3bEhQsXcP78ed3SokULDBkyBOfPn4elpaXJ9yHPkydPcOPGDTg7O5u81bZt2wIfm/vXX3+hbt26Jm/nt2bNGlSrVg3du3cXpJeVlQULC/2XOktLS91fI4Via2sLZ2dnpKWl4cCBA+jVq5eg/fr166NGjRp6r3MZGRmIjY0t169zwP8PFq5du4Zff/0VCoVC1P0R4rVu6NChSEhI0Hudq1mzJsLCwnDgwAGTtgvzzz//QKlUmvS1ztraGi1btiwTr3OrVq1C8+bNBTlX5enTp3j69Knor3P29vZ45ZVXcO3aNZw5c0bw1zgyLb4l6T+hoaEIDg5GixYt0KpVK3z99dfIzMzEiBEjTN5+8uSJ3l9dEhMTcf78eVStWhV16tQxWTckJASbN2/Gjz/+iCpVqujex2xvb4+KFSuarAsA4eHhCAwMRJ06dfD48WNs3rwZR48eFeQ/ZFWqVClwnoatrS0UCoXJz9+YOHEievbsibp16+LevXuIiIiApaUlBg8ebNIuAEyYMAFt2rTBl19+iQEDBuDUqVNYsWIFVqxYYfJ2Ho1GgzVr1iA4OBgVKgjz8tOzZ0/873//Q506deDp6Ylz587hq6++wrvvvitI/8CBA9BqtXBzc8P169cRFhYGd3d3k7y2vOi1ZPz48fjiiy/QsGFD1K9fH9OmTUPNmjURFBRk0u7Dhw9x+/Zt3bUP8n6hq1GjRqnMbhTVd3Z2Rr9+/RAXF4c9e/ZArVbrXuuqVq0Ka2trk7UVCgX+97//4a233oKzszNSU1OxePFi3L17t1Q+TvhFx/35gZGVlRVq1KgBNzc3k7arVq2KyMhI9O3bFzVq1MCNGzfw6aefwtXVFQEBASbr1qlTB2FhYRg4cCDat2+PN954A/v378dPP/2Eo0ePvlS3uH3g34H4tm3bMG/evFJpFqfr7++PsLAwVKxYEXXr1sWxY8ewfv16fPXVVyZvb9u2Da+88grq1KmDCxcuYNy4cQgKCjL5CdckMFE/o6mM+eabb7R16tTRWltba1u1aqU9efKkIN0jR45oARRYgoODTdotrAlAu2bNGpN2tVqt9t1339XWrVtXa21trX3llVe0HTt21P7yyy8m7xoi1MeqDhw4UOvs7Ky1trbW1qpVSztw4EDt9evXTd7N89NPP2kbN26slcvlWnd3d+2KFSsEa2u1Wu2BAwe0ALRXr14VrJmRkaEdN26ctk6dOlobGxttgwYNtFOmTNGqVCpB+lu3btU2aNBAa21tra1Ro4Y2JCRE++jRI5O0XvRaotFotNOmTdNWr15dK5fLtR07diyV5+JF3TVr1hR6e0RExEu3X9TP+yjXwpYjR46YtJ2dna3t3bu3tmbNmlpra2uts7Oz9q233tKeOnXq5R/0C9qFKc2PVS2qnZWVpe3SpYv2lVde0VpZWWnr1q2rfe+997TJyckm7eZZtWqV1tXVVWtjY6P19vbW7tq166W7xvSXL1+urVixYqn+O39RNykpSTt8+HBtzZo1tTY2Nlo3NzftvHnzSuVjq1/UXrBggbZ27dpaKysrbZ06dbRTp04V7PWVhCPTanlWChERERERFY7nMBARERERkUEcMBARERERkUEcMBARERH9X3v3FhLl1ocB/JmymcbDYGNmnhU0G0EmLRC70Cwjr3KnoXQca8hMJSst80LKzIxKOhF6kykd6GRJqCASmZIpWWgXlaUoWnRRYsGYzujM+m5278dkU7Pb1d66nx/MxbvWmrX+71yIz6z1KhHZxMBAREREREQ2MTAQEREREZFNDAxERERERGQTAwMREREREdnEwEBERERERDYxMBAR/U2pqan4448/pOtly5Zh165dv72OpqYmyGQyfPjw4Zet8eW9/ojfUScREf08DAxENC2lpqZCJpNBJpNBLpcjKCgIhw4dwsTExC9f+9atWygqKrJr7O/+5TkgIACnTp36LWsREdH04PBPF0BE9KvEx8fjwoULMBqNqK+vR2ZmJmbNmoX8/PxJY00mE+Ry+U9ZV61W/5R5iIiI/g24w0BE05ZCocD8+fPh7++PHTt2IC4uDnfu3AHw/6M1xcXF8PLyQkhICABgcHAQycnJcHV1hVqtRkJCAvr7+6U5zWYz9uzZA1dXV7i5uWHfvn0QQlit++WRJKPRiLy8PPj6+kKhUCAoKAjnz59Hf38/YmNjAQBz5syBTCZDamoqAMBisaCkpASBgYFQKpXQarW4efOm1Tr19fVYsGABlEolYmNjrer8EWazGXq9XlozJCQEp0+f/urYwsJCuLu7Q6VSIT09HSaTSeqzp3YiIpo6uMNARP8ZSqUSQ0ND0vXdu3ehUqnQ2NgIABgfH8eqVasQFRWFlpYWODg44PDhw4iPj8fTp08hl8tRWlqKyspKVFRUQKPRoLS0FLdv38by5cttrrt582Y8fPgQZ86cgVarRV9fH96/fw9fX19UV1cjKSkJ3d3dUKlUUCqVAICSkhJcunQJ5eXlCA4ORnNzMzZu3Ah3d3fExMRgcHAQiYmJyMzMRFpaGjo6OpCTk/O3Ph+LxQIfHx/cuHEDbm5uaG1tRVpaGjw9PZGcnGz1uc2ePRtNTU3o7+/Hli1b4ObmhuLiYrtqJyKiKUYQEU1DOp1OJCQkCCGEsFgsorGxUSgUCpGbmyv1e3h4CKPRKL3n4sWLIiQkRFgsFqnNaDQKpVIpGhoahBBCeHp6imPHjkn94+PjwsfHR1pLCCFiYmJEdna2EEKI7u5uAUA0NjZ+tc579+4JAGJ4eFhqGxsbE46OjqK1tdVqrF6vF+vWrRNCCJGfny9CQ0Ot+vPy8ibN9SV/f39x8uRJm/1fyszMFElJSdK1TqcTarVajIyMSG1lZWXC2dlZmM1mu2r/2j0TEdG/F3cYiGjaqq2thbOzM8bHx2GxWLB+/XocPHhQ6g8LC7N6bqGrqws9PT1wcXGxmmdsbAy9vb34+PEj3r59i8jISKnPwcEBS5YsmXQs6bPOzk7MnDnzL32z3tPTg0+fPmHlypVW7SaTCeHh4QCA58+fW9UBAFFRUXavYcu5c+dQUVGBgYEBjI6OwmQyYdGiRVZjtFotHB0drdY1GAwYHByEwWD4bu1ERDS1MDAQ0bQVGxuLsrIyyOVyeHl5wcHB+keek5OT1bXBYMDixYtx+fLlSXO5u7v/UA2fjxj9FQaDAQBQV1cHb29vqz6FQvFDddjj6tWryM3NRWlpKaKiouDi4oLjx4+jvb3d7jn+qdqJiOjXYWAgomnLyckJQUFBdo+PiIjAtWvXMG/ePKhUqq+O8fT0RHt7O6KjowEAExMTePz4MSIiIr46PiwsDBaLBffv30dcXNyk/s87HGazWWoLDQ2FQqHAwMCAzZ0JjUYjPcD9WVtb2/dv8hsePHiApUuXIiMjQ2rr7e2dNK6rqwujo6NSGGpra4OzszN8fX2hVqu/WzsREU0t/CtJRER/2rBhA+bOnYuEhAS0tLSgr68PTU1N2LlzJ16/fg0AyM7OxtGjR1FTU4MXL14gIyPjm/9DISAgADqdDlu3bkVNTY005/Xr1wEA/v7+kMlkqK2txbt372AwGODi4oLc3Fzs3r0bVVVV6O3txZMnT3D27FlUVVUBANLT0/Hq1Svs3bsX3d3duHLlCiorK+26zzdv3qCzs9PqNTw8jODgYHR0dKChoQEvX75EQUEBHj16NOn9JpMJer0ez549Q319PQ4cOICsrCzMmDHDrtqJiGhqYWAgIvqTo6Mjmpub4efnh8TERGg0Guj1eoyNjUk7Djk5Odi0aRN0Op10bGfNmjXfnLesrAxr165FRkYGFi5ciG3btmFkZAQA4O3tjcLCQuzfvx8eHh7IysoCABQVFaGgoAAlJSXQaDSIj49HXV0dAgMDAQB+fn6orq5GTU0NtFotysvLceTIEbvu88SJEwgPD7d61dXVYfv27UhMTERKSgoiIyMxNDRktdvw2YoVKxAcHIzo6GikpKRg9erVVs+GfK92IiKaWmTC1pN6RERERET0n8cdBiIiIiIisomBgYiIiIiIbGJgICIiIiIimxgYiIiIiIjIJgYGIiIiIiKyiYGBiIiIiIhsYmAgIiIiIiKbGBiIiIiIiMgmBgYiIiIiIrKJgYGIiIiIiGxiYCAiIiIiIpv+B5pjNBzts21bAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "mmbra.evaluation_visualization_example(test_label_np, test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l71C31hhLg_"
      },
      "source": [
        "Precision, Recall, and F1-Score(**example**)\n",
        "\n",
        "In this section, we are calculating key **classification performance metrics**: precision, recall, and F1-score, which provide a deeper insight into the model's performance beyond accuracy.\n",
        "   \n",
        "This step gives a deeper evaluation of the model's performance by considering both the correctness of the positive predictions (precision) and the ability to capture all actual positives (recall), as well as their balance (F1-score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVdaWLkuhWRS",
        "outputId": "3f897ee4-e51d-4b85-9e8c-61db3ee46001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3967\n",
            "Recall: 0.3667\n",
            "F1-Score: 0.3617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "mmbra.diverse_evaluation_metrics_example(test_label_np, test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OkKnAVra9agp"
      },
      "outputs": [],
      "source": [
        "# TODO: try different evaluation methods/metrics to evaluate machine learning or deep learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R_cVbUe6_58"
      },
      "source": [
        "Baseline Results:\n",
        "\n",
        "SVM(linear): Accuracy on test data: 0.23\n",
        "\n",
        "RandomForestClassifier(n_estimators=100, random_state=42): Accuracy on test data: 0.16\n",
        "\n",
        "KNeighborsClassifier(n_neighbors=5): Accuracy on test data: 0.15\n",
        "\n",
        "LogisticRegression(random_state=42, max_iter=1000): Accuracy on test data: 0.36"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k-kvTFW_3_FH",
        "KPAlCPa5fTtY",
        "-N9UtHCl8SOu"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}